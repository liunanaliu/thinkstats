<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Statistical Thinking for the 21st Century</title>
  <meta name="description" content="Statistical Thinking for the 21st Century">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Statistical Thinking for the 21st Century" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="poldrack/psych10-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistical Thinking for the 21st Century" />
  
  
  

<meta name="author" content="Copyright 2018 Russell A. Poldrack">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="resampling-and-simulation.html">
<link rel="next" href="ci-effect-size-power.html">
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="book_assets/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="book_assets/viz-0.3/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.0/grViz.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129414074-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129414074-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#why-does-this-book-exist"><i class="fa fa-check"></i><b>0.1</b> Why does this book exist?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#youre-not-a-statistician---why-should-we-listen-to-you"><i class="fa fa-check"></i><b>0.2</b> You’re not a statistician - why should we listen to you?</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#why-r"><i class="fa fa-check"></i><b>0.3</b> Why R?</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#the-golden-age-of-data"><i class="fa fa-check"></i><b>0.4</b> The golden age of data</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#an-open-source-book"><i class="fa fa-check"></i><b>0.5</b> An open source book</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.6</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-statistical-thinking"><i class="fa fa-check"></i><b>1.1</b> What is statistical thinking?</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#what-can-statistics-do-for-us"><i class="fa fa-check"></i><b>1.2</b> What can statistics do for us?</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#fundamental-concepts-of-statistics"><i class="fa fa-check"></i><b>1.3</b> Fundamental concepts of statistics</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#learning-from-data"><i class="fa fa-check"></i><b>1.3.1</b> Learning from data</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#aggregation"><i class="fa fa-check"></i><b>1.3.2</b> Aggregation</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#uncertainty"><i class="fa fa-check"></i><b>1.3.3</b> Uncertainty</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#sampling"><i class="fa fa-check"></i><b>1.3.4</b> Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#causality-and-statistics"><i class="fa fa-check"></i><b>1.4</b> Causality and statistics</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#suggested-readings"><i class="fa fa-check"></i><b>1.5</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="working-with-data.html"><a href="working-with-data.html"><i class="fa fa-check"></i><b>2</b> Working with data</a><ul>
<li class="chapter" data-level="2.1" data-path="working-with-data.html"><a href="working-with-data.html#what-are-data"><i class="fa fa-check"></i><b>2.1</b> What are data?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="working-with-data.html"><a href="working-with-data.html#qualitative-data"><i class="fa fa-check"></i><b>2.1.1</b> Qualitative data</a></li>
<li class="chapter" data-level="2.1.2" data-path="working-with-data.html"><a href="working-with-data.html#quantitative-data"><i class="fa fa-check"></i><b>2.1.2</b> Quantitative data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="working-with-data.html"><a href="working-with-data.html#scales-of-measurement"><i class="fa fa-check"></i><b>2.2</b> Scales of measurement</a><ul>
<li class="chapter" data-level="2.2.1" data-path="working-with-data.html"><a href="working-with-data.html#why-do-scales-of-measurement-matter"><i class="fa fa-check"></i><b>2.2.1</b> Why do scales of measurement matter?</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="working-with-data.html"><a href="working-with-data.html#what-makes-a-good-measurement"><i class="fa fa-check"></i><b>2.3</b> What makes a good measurement?</a><ul>
<li class="chapter" data-level="2.3.1" data-path="working-with-data.html"><a href="working-with-data.html#reliability"><i class="fa fa-check"></i><b>2.3.1</b> <em>Reliability</em></a></li>
<li class="chapter" data-level="2.3.2" data-path="working-with-data.html"><a href="working-with-data.html#validity"><i class="fa fa-check"></i><b>2.3.2</b> <em>Validity</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="working-with-data.html"><a href="working-with-data.html#suggested-readings-1"><i class="fa fa-check"></i><b>2.4</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a><ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#what-is-probability"><i class="fa fa-check"></i><b>3.1</b> What is probability?</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#how-do-we-determine-probabilities"><i class="fa fa-check"></i><b>3.2</b> How do we determine probabilities?</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability.html"><a href="probability.html#personal-opinion"><i class="fa fa-check"></i><b>3.2.1</b> Personal opinion</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability.html"><a href="probability.html#empirical-frequency"><i class="fa fa-check"></i><b>3.2.2</b> Empirical frequency</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability.html"><a href="probability.html#classical-probability"><i class="fa fa-check"></i><b>3.2.3</b> Classical probability</a></li>
<li class="chapter" data-level="3.2.4" data-path="probability.html"><a href="probability.html#solving-de-meres-problem"><i class="fa fa-check"></i><b>3.2.4</b> Solving de Méré’s problem</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>3.3</b> Probability distributions</a><ul>
<li class="chapter" data-level="3.3.1" data-path="probability.html"><a href="probability.html#cumulative-probability-distributions"><i class="fa fa-check"></i><b>3.3.1</b> Cumulative probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.4</b> Conditional probability</a></li>
<li class="chapter" data-level="3.5" data-path="probability.html"><a href="probability.html#computing-conditional-probabilities-from-data"><i class="fa fa-check"></i><b>3.5</b> Computing conditional probabilities from data</a></li>
<li class="chapter" data-level="3.6" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>3.6</b> Independence</a></li>
<li class="chapter" data-level="3.7" data-path="probability.html"><a href="probability.html#bayestheorem"><i class="fa fa-check"></i><b>3.7</b> Reversing a conditional probability: Bayes’ rule</a></li>
<li class="chapter" data-level="3.8" data-path="probability.html"><a href="probability.html#learning-from-data-1"><i class="fa fa-check"></i><b>3.8</b> Learning from data</a></li>
<li class="chapter" data-level="3.9" data-path="probability.html"><a href="probability.html#odds-and-odds-ratios"><i class="fa fa-check"></i><b>3.9</b> Odds and odds ratios</a></li>
<li class="chapter" data-level="3.10" data-path="probability.html"><a href="probability.html#what-do-probabilities-mean"><i class="fa fa-check"></i><b>3.10</b> What do probabilities mean?</a></li>
<li class="chapter" data-level="3.11" data-path="probability.html"><a href="probability.html#suggested-readings-2"><i class="fa fa-check"></i><b>3.11</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="summarizing-data.html"><a href="summarizing-data.html"><i class="fa fa-check"></i><b>4</b> Summarizing data</a><ul>
<li class="chapter" data-level="4.1" data-path="summarizing-data.html"><a href="summarizing-data.html#why-summarize-data"><i class="fa fa-check"></i><b>4.1</b> Why summarize data?</a></li>
<li class="chapter" data-level="4.2" data-path="summarizing-data.html"><a href="summarizing-data.html#summarizing-data-using-tables"><i class="fa fa-check"></i><b>4.2</b> Summarizing data using tables</a><ul>
<li class="chapter" data-level="4.2.1" data-path="summarizing-data.html"><a href="summarizing-data.html#frequency-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Frequency distributions</a></li>
<li class="chapter" data-level="4.2.2" data-path="summarizing-data.html"><a href="summarizing-data.html#cumulative-distributions"><i class="fa fa-check"></i><b>4.2.2</b> Cumulative distributions</a></li>
<li class="chapter" data-level="4.2.3" data-path="summarizing-data.html"><a href="summarizing-data.html#plotting-histograms"><i class="fa fa-check"></i><b>4.2.3</b> Plotting histograms</a></li>
<li class="chapter" data-level="4.2.4" data-path="summarizing-data.html"><a href="summarizing-data.html#histogram-bins"><i class="fa fa-check"></i><b>4.2.4</b> Histogram bins</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="summarizing-data.html"><a href="summarizing-data.html#idealized-representations-of-distributions"><i class="fa fa-check"></i><b>4.3</b> Idealized representations of distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="summarizing-data.html"><a href="summarizing-data.html#skewness"><i class="fa fa-check"></i><b>4.3.1</b> Skewness</a></li>
<li class="chapter" data-level="4.3.2" data-path="summarizing-data.html"><a href="summarizing-data.html#long-tailed-distributions"><i class="fa fa-check"></i><b>4.3.2</b> Long-tailed distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summarizing-data.html"><a href="summarizing-data.html#suggested-readings-3"><i class="fa fa-check"></i><b>4.4</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html"><i class="fa fa-check"></i><b>5</b> Fitting models to data</a><ul>
<li class="chapter" data-level="5.1" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#what-is-a-model"><i class="fa fa-check"></i><b>5.1</b> What is a model?</a></li>
<li class="chapter" data-level="5.2" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#statistical-modeling-an-example"><i class="fa fa-check"></i><b>5.2</b> Statistical modeling: An example</a><ul>
<li class="chapter" data-level="5.2.1" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#improving-our-model"><i class="fa fa-check"></i><b>5.2.1</b> Improving our model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#what-makes-a-model-good"><i class="fa fa-check"></i><b>5.3</b> What makes a model “good”?</a></li>
<li class="chapter" data-level="5.4" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#overfitting"><i class="fa fa-check"></i><b>5.4</b> Can a model be too good?</a></li>
<li class="chapter" data-level="5.5" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#the-simplest-model-the-mean"><i class="fa fa-check"></i><b>5.5</b> The simplest model: The mean</a><ul>
<li class="chapter" data-level="5.5.1" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#the-median"><i class="fa fa-check"></i><b>5.5.1</b> The median</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#the-mode"><i class="fa fa-check"></i><b>5.6</b> The mode</a></li>
<li class="chapter" data-level="5.7" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#variability-how-well-does-the-mean-fit-the-data"><i class="fa fa-check"></i><b>5.7</b> Variability: How well does the mean fit the data?</a></li>
<li class="chapter" data-level="5.8" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#using-simulations-to-understand-statistics"><i class="fa fa-check"></i><b>5.8</b> Using simulations to understand statistics</a></li>
<li class="chapter" data-level="5.9" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#z-scores"><i class="fa fa-check"></i><b>5.9</b> Z-scores</a><ul>
<li class="chapter" data-level="5.9.1" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#interpreting-z-scores"><i class="fa fa-check"></i><b>5.9.1</b> Interpreting Z-scores</a></li>
<li class="chapter" data-level="5.9.2" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#standardized-scores"><i class="fa fa-check"></i><b>5.9.2</b> Standardized scores</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>6</b> Data Visualization</a><ul>
<li class="chapter" data-level="6.1" data-path="data-visualization.html"><a href="data-visualization.html#how-data-visualization-can-save-lives"><i class="fa fa-check"></i><b>6.1</b> How data visualization can save lives</a></li>
<li class="chapter" data-level="6.2" data-path="data-visualization.html"><a href="data-visualization.html#anatomy-of-a-plot"><i class="fa fa-check"></i><b>6.2</b> Anatomy of a plot</a></li>
<li class="chapter" data-level="6.3" data-path="data-visualization.html"><a href="data-visualization.html#plotting-in-r-using-ggplot"><i class="fa fa-check"></i><b>6.3</b> Plotting in R using ggplot</a></li>
<li class="chapter" data-level="6.4" data-path="data-visualization.html"><a href="data-visualization.html#principles-of-good-visualization"><i class="fa fa-check"></i><b>6.4</b> Principles of good visualization</a><ul>
<li class="chapter" data-level="6.4.1" data-path="data-visualization.html"><a href="data-visualization.html#show-the-data-and-make-them-stand-out"><i class="fa fa-check"></i><b>6.4.1</b> Show the data and make them stand out</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="data-visualization.html"><a href="data-visualization.html#maximize-the-dataink-ratio"><i class="fa fa-check"></i><b>6.5</b> Maximize the data/ink ratio</a></li>
<li class="chapter" data-level="6.6" data-path="data-visualization.html"><a href="data-visualization.html#avoid-chartjunk"><i class="fa fa-check"></i><b>6.6</b> Avoid chartjunk</a></li>
<li class="chapter" data-level="6.7" data-path="data-visualization.html"><a href="data-visualization.html#avoid-distorting-the-data"><i class="fa fa-check"></i><b>6.7</b> Avoid distorting the data</a></li>
<li class="chapter" data-level="6.8" data-path="data-visualization.html"><a href="data-visualization.html#the-lie-factor"><i class="fa fa-check"></i><b>6.8</b> The lie factor</a></li>
<li class="chapter" data-level="6.9" data-path="data-visualization.html"><a href="data-visualization.html#remember-human-limitations"><i class="fa fa-check"></i><b>6.9</b> Remember human limitations</a><ul>
<li class="chapter" data-level="6.9.1" data-path="data-visualization.html"><a href="data-visualization.html#perceptual-limitations"><i class="fa fa-check"></i><b>6.9.1</b> Perceptual limitations</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="data-visualization.html"><a href="data-visualization.html#correcting-for-other-factors"><i class="fa fa-check"></i><b>6.10</b> Correcting for other factors</a></li>
<li class="chapter" data-level="6.11" data-path="data-visualization.html"><a href="data-visualization.html#suggested-readings-and-videos"><i class="fa fa-check"></i><b>6.11</b> Suggested readings and videos</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sampling-1.html"><a href="sampling-1.html"><i class="fa fa-check"></i><b>7</b> Sampling</a><ul>
<li class="chapter" data-level="7.1" data-path="sampling-1.html"><a href="sampling-1.html#how-do-we-sample"><i class="fa fa-check"></i><b>7.1</b> How do we sample?</a></li>
<li class="chapter" data-level="7.2" data-path="sampling-1.html"><a href="sampling-1.html#sampling-error"><i class="fa fa-check"></i><b>7.2</b> Sampling error</a></li>
<li class="chapter" data-level="7.3" data-path="sampling-1.html"><a href="sampling-1.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>7.3</b> Standard error of the mean</a></li>
<li class="chapter" data-level="7.4" data-path="sampling-1.html"><a href="sampling-1.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>7.4</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.5" data-path="sampling-1.html"><a href="sampling-1.html#confidence-intervals"><i class="fa fa-check"></i><b>7.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="7.6" data-path="sampling-1.html"><a href="sampling-1.html#suggested-readings-4"><i class="fa fa-check"></i><b>7.6</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html"><i class="fa fa-check"></i><b>8</b> Resampling and simulation</a><ul>
<li class="chapter" data-level="8.1" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.1</b> Monte Carlo simulation</a></li>
<li class="chapter" data-level="8.2" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#randomness-in-statistics"><i class="fa fa-check"></i><b>8.2</b> Randomness in statistics</a></li>
<li class="chapter" data-level="8.3" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#generating-random-numbers"><i class="fa fa-check"></i><b>8.3</b> Generating random numbers</a></li>
<li class="chapter" data-level="8.4" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#using-monte-carlo-simulation"><i class="fa fa-check"></i><b>8.4</b> Using Monte Carlo simulation</a></li>
<li class="chapter" data-level="8.5" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#using-simulation-for-statistics-the-bootstrap"><i class="fa fa-check"></i><b>8.5</b> Using simulation for statistics: The bootstrap</a><ul>
<li class="chapter" data-level="8.5.1" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#computing-the-bootstrap"><i class="fa fa-check"></i><b>8.5.1</b> Computing the bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#suggested-readings-5"><i class="fa fa-check"></i><b>8.6</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>9</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="9.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#null-hypothesis-statistical-testing-nhst"><i class="fa fa-check"></i><b>9.1</b> Null Hypothesis Statistical Testing (NHST)</a></li>
<li class="chapter" data-level="9.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#null-hypothesis-statistical-testing-an-example"><i class="fa fa-check"></i><b>9.2</b> Null hypothesis statistical testing: An example</a></li>
<li class="chapter" data-level="9.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#the-process-of-null-hypothesis-testing"><i class="fa fa-check"></i><b>9.3</b> The process of null hypothesis testing</a><ul>
<li class="chapter" data-level="9.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-1-formulate-a-hypothesis"><i class="fa fa-check"></i><b>9.3.1</b> Step 1: Formulate a hypothesis</a></li>
<li class="chapter" data-level="9.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-2-collect-some-data"><i class="fa fa-check"></i><b>9.3.2</b> Step 2: Collect some data</a></li>
<li class="chapter" data-level="9.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-3-specify-the-null-and-alternative-hypotheses"><i class="fa fa-check"></i><b>9.3.3</b> Step 3: Specify the null and alternative hypotheses</a></li>
<li class="chapter" data-level="9.3.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-4-fit-a-model-to-the-data-and-compute-a-test-statistic"><i class="fa fa-check"></i><b>9.3.4</b> Step 4: Fit a model to the data and compute a test statistic</a></li>
<li class="chapter" data-level="9.3.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-5-determine-the-probability-of-the-data-under-the-null-hypothesis"><i class="fa fa-check"></i><b>9.3.5</b> Step 5: Determine the probability of the data under the null hypothesis</a></li>
<li class="chapter" data-level="9.3.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-6-assess-the-statistical-significance-of-the-result"><i class="fa fa-check"></i><b>9.3.6</b> Step 6: Assess the “statistical significance” of the result</a></li>
<li class="chapter" data-level="9.3.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#what-does-a-significant-result-mean"><i class="fa fa-check"></i><b>9.3.7</b> What does a significant result mean?</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#nhst-in-a-modern-context-multiple-testing"><i class="fa fa-check"></i><b>9.4</b> NHST in a modern context: Multiple testing</a></li>
<li class="chapter" data-level="9.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#suggested-readings-6"><i class="fa fa-check"></i><b>9.5</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html"><i class="fa fa-check"></i><b>10</b> Confidence intervals, effect sizes, and statistical power</a><ul>
<li class="chapter" data-level="10.1" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#confidence-intervals-1"><i class="fa fa-check"></i><b>10.1</b> Confidence intervals</a><ul>
<li class="chapter" data-level="10.1.1" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#confidence-intervals-using-the-normal-distribution"><i class="fa fa-check"></i><b>10.1.1</b> Confidence intervals using the normal distribution</a></li>
<li class="chapter" data-level="10.1.2" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#confidence-intervals-using-the-t-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Confidence intervals using the t distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#confidence-intervals-and-sample-size"><i class="fa fa-check"></i><b>10.1.3</b> Confidence intervals and sample size</a></li>
<li class="chapter" data-level="10.1.4" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#computing-confidence-intervals-using-the-bootstrap"><i class="fa fa-check"></i><b>10.1.4</b> Computing confidence intervals using the bootstrap</a></li>
<li class="chapter" data-level="10.1.5" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#relation-of-confidence-intervals-to-hypothesis-tests"><i class="fa fa-check"></i><b>10.1.5</b> Relation of confidence intervals to hypothesis tests</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#effect-sizes"><i class="fa fa-check"></i><b>10.2</b> Effect sizes</a><ul>
<li class="chapter" data-level="10.2.1" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#cohens-d"><i class="fa fa-check"></i><b>10.2.1</b> Cohen’s D</a></li>
<li class="chapter" data-level="10.2.2" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#pearsons-r"><i class="fa fa-check"></i><b>10.2.2</b> Pearson’s r</a></li>
<li class="chapter" data-level="10.2.3" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#odds-ratio"><i class="fa fa-check"></i><b>10.2.3</b> Odds ratio</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#statistical-power"><i class="fa fa-check"></i><b>10.3</b> Statistical power</a><ul>
<li class="chapter" data-level="10.3.1" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#power-analysis"><i class="fa fa-check"></i><b>10.3.1</b> Power analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#suggested-readings-7"><i class="fa fa-check"></i><b>10.4</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html"><i class="fa fa-check"></i><b>11</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="11.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#generative-models"><i class="fa fa-check"></i><b>11.1</b> Generative models</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayes-theorem-and-inverse-inference"><i class="fa fa-check"></i><b>11.2</b> Bayes’ theorem and inverse inference</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#doing-bayesian-estimation"><i class="fa fa-check"></i><b>11.3</b> Doing Bayesian estimation</a><ul>
<li class="chapter" data-level="11.3.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#specifying-the-prior"><i class="fa fa-check"></i><b>11.3.1</b> Specifying the prior</a></li>
<li class="chapter" data-level="11.3.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#collect-some-data"><i class="fa fa-check"></i><b>11.3.2</b> Collect some data</a></li>
<li class="chapter" data-level="11.3.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-likelihood"><i class="fa fa-check"></i><b>11.3.3</b> Computing the likelihood</a></li>
<li class="chapter" data-level="11.3.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-marginal-likelihood"><i class="fa fa-check"></i><b>11.3.4</b> Computing the marginal likelihood</a></li>
<li class="chapter" data-level="11.3.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-posterior"><i class="fa fa-check"></i><b>11.3.5</b> Computing the posterior</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#estimating-posterior-distributions"><i class="fa fa-check"></i><b>11.4</b> Estimating posterior distributions</a><ul>
<li class="chapter" data-level="11.4.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#specifying-the-prior-1"><i class="fa fa-check"></i><b>11.4.1</b> Specifying the prior</a></li>
<li class="chapter" data-level="11.4.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#collect-some-data-1"><i class="fa fa-check"></i><b>11.4.2</b> Collect some data</a></li>
<li class="chapter" data-level="11.4.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-likelihood-1"><i class="fa fa-check"></i><b>11.4.3</b> Computing the likelihood</a></li>
<li class="chapter" data-level="11.4.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-marginal-likelihood-1"><i class="fa fa-check"></i><b>11.4.4</b> Computing the marginal likelihood</a></li>
<li class="chapter" data-level="11.4.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-posterior-1"><i class="fa fa-check"></i><b>11.4.5</b> Computing the posterior</a></li>
<li class="chapter" data-level="11.4.6" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#maximum-a-posteriori-map-estimation"><i class="fa fa-check"></i><b>11.4.6</b> Maximum a posteriori (MAP) estimation</a></li>
<li class="chapter" data-level="11.4.7" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#credible-intervals"><i class="fa fa-check"></i><b>11.4.7</b> Credible intervals</a></li>
<li class="chapter" data-level="11.4.8" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#effects-of-different-priors"><i class="fa fa-check"></i><b>11.4.8</b> Effects of different priors</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#choosing-a-prior"><i class="fa fa-check"></i><b>11.5</b> Choosing a prior</a></li>
<li class="chapter" data-level="11.6" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayesian-hypothesis-testing"><i class="fa fa-check"></i><b>11.6</b> Bayesian hypothesis testing</a><ul>
<li class="chapter" data-level="11.6.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayes-factors"><i class="fa fa-check"></i><b>11.6.1</b> Bayes factors</a></li>
<li class="chapter" data-level="11.6.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayes-factors-for-statistical-hypotheses"><i class="fa fa-check"></i><b>11.6.2</b> Bayes factors for statistical hypotheses</a></li>
<li class="chapter" data-level="11.6.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#assessing-evidence-for-the-null-hypothesis"><i class="fa fa-check"></i><b>11.6.3</b> Assessing evidence for the null hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#suggested-readings-8"><i class="fa fa-check"></i><b>11.7</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html"><i class="fa fa-check"></i><b>12</b> Modeling categorical relationships</a><ul>
<li class="chapter" data-level="12.1" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#example-candy-colors"><i class="fa fa-check"></i><b>12.1</b> Example: Candy colors</a></li>
<li class="chapter" data-level="12.2" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#pearsons-chi-squared-test"><i class="fa fa-check"></i><b>12.2</b> Pearson’s chi-squared test</a></li>
<li class="chapter" data-level="12.3" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#contingency-tables-and-the-two-way-test"><i class="fa fa-check"></i><b>12.3</b> Contingency tables and the two-way test</a></li>
<li class="chapter" data-level="12.4" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#standardized-residuals"><i class="fa fa-check"></i><b>12.4</b> Standardized residuals</a></li>
<li class="chapter" data-level="12.5" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#odds-ratios"><i class="fa fa-check"></i><b>12.5</b> Odds ratios</a></li>
<li class="chapter" data-level="12.6" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#bayes-factor"><i class="fa fa-check"></i><b>12.6</b> Bayes factor</a></li>
<li class="chapter" data-level="12.7" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#categorical-analysis-beyond-the-2-x-2-table"><i class="fa fa-check"></i><b>12.7</b> Categorical analysis beyond the 2 X 2 table</a></li>
<li class="chapter" data-level="12.8" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#beware-of-simpsons-paradox"><i class="fa fa-check"></i><b>12.8</b> Beware of Simpson’s paradox</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html"><i class="fa fa-check"></i><b>13</b> Modeling continuous relationships</a><ul>
<li class="chapter" data-level="13.1" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#an-example-hate-crimes-and-income-inequality"><i class="fa fa-check"></i><b>13.1</b> An example: Hate crimes and income inequality</a><ul>
<li class="chapter" data-level="13.1.1" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#quantifying-inequality-the-gini-index"><i class="fa fa-check"></i><b>13.1.1</b> Quantifying inequality: The Gini index</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#is-income-inequality-related-to-hate-crimes"><i class="fa fa-check"></i><b>13.2</b> Is income inequality related to hate crimes?</a></li>
<li class="chapter" data-level="13.3" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#covariance-and-correlation"><i class="fa fa-check"></i><b>13.3</b> Covariance and correlation</a><ul>
<li class="chapter" data-level="13.3.1" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#hypothesis-testing-for-correlations"><i class="fa fa-check"></i><b>13.3.1</b> Hypothesis testing for correlations</a></li>
<li class="chapter" data-level="13.3.2" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#robust-correlations"><i class="fa fa-check"></i><b>13.3.2</b> Robust correlations</a></li>
<li class="chapter" data-level="13.3.3" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#bayesian-correlation-analysis"><i class="fa fa-check"></i><b>13.3.3</b> Bayesian correlation analysis</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#correlation-and-causation"><i class="fa fa-check"></i><b>13.4</b> Correlation and causation</a><ul>
<li class="chapter" data-level="13.4.1" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#causal-graphs"><i class="fa fa-check"></i><b>13.4.1</b> Causal graphs</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#suggested-readings-9"><i class="fa fa-check"></i><b>13.5</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html"><i class="fa fa-check"></i><b>14</b> The General Linear Model</a><ul>
<li class="chapter" data-level="14.1" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#linear-regression"><i class="fa fa-check"></i><b>14.1</b> Linear regression</a><ul>
<li class="chapter" data-level="14.1.1" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#regression-to-the-mean"><i class="fa fa-check"></i><b>14.1.1</b> Regression to the mean</a></li>
<li class="chapter" data-level="14.1.2" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#estimating-linear-regression-parameters"><i class="fa fa-check"></i><b>14.1.2</b> Estimating linear regression parameters</a></li>
<li class="chapter" data-level="14.1.3" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#the-relation-between-correlation-and-regression"><i class="fa fa-check"></i><b>14.1.3</b> The relation between correlation and regression</a></li>
<li class="chapter" data-level="14.1.4" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#standard-errors-for-regression-models"><i class="fa fa-check"></i><b>14.1.4</b> Standard errors for regression models</a></li>
<li class="chapter" data-level="14.1.5" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#statistical-tests-for-regression-parameters"><i class="fa fa-check"></i><b>14.1.5</b> Statistical tests for regression parameters</a></li>
<li class="chapter" data-level="14.1.6" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#quantifying-goodness-of-fit-of-the-model"><i class="fa fa-check"></i><b>14.1.6</b> Quantifying goodness of fit of the model</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#fitting-more-complex-models"><i class="fa fa-check"></i><b>14.2</b> Fitting more complex models</a></li>
<li class="chapter" data-level="14.3" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#interactions-between-variables"><i class="fa fa-check"></i><b>14.3</b> Interactions between variables</a></li>
<li class="chapter" data-level="14.4" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#what-does-predict-really-mean"><i class="fa fa-check"></i><b>14.4</b> What does “predict” really mean?</a><ul>
<li class="chapter" data-level="14.4.1" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#cross-validation"><i class="fa fa-check"></i><b>14.4.1</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#suggested-readings-10"><i class="fa fa-check"></i><b>14.5</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="comparing-means.html"><a href="comparing-means.html"><i class="fa fa-check"></i><b>15</b> Comparing means</a><ul>
<li class="chapter" data-level="15.1" data-path="comparing-means.html"><a href="comparing-means.html#students-t-test"><i class="fa fa-check"></i><b>15.1</b> Student’s T test</a></li>
<li class="chapter" data-level="15.2" data-path="comparing-means.html"><a href="comparing-means.html#the-t-test-as-a-linear-model"><i class="fa fa-check"></i><b>15.2</b> The t-test as a linear model</a><ul>
<li class="chapter" data-level="15.2.1" data-path="comparing-means.html"><a href="comparing-means.html#effect-sizes-for-comparing-two-means"><i class="fa fa-check"></i><b>15.2.1</b> Effect sizes for comparing two means</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="comparing-means.html"><a href="comparing-means.html#bayes-factor-for-mean-differences"><i class="fa fa-check"></i><b>15.3</b> Bayes factor for mean differences</a></li>
<li class="chapter" data-level="15.4" data-path="comparing-means.html"><a href="comparing-means.html#paired-t-tests"><i class="fa fa-check"></i><b>15.4</b> Paired t-tests</a><ul>
<li class="chapter" data-level="15.4.1" data-path="comparing-means.html"><a href="comparing-means.html#sign-test"><i class="fa fa-check"></i><b>15.4.1</b> Sign test</a></li>
<li class="chapter" data-level="15.4.2" data-path="comparing-means.html"><a href="comparing-means.html#paired-t-test"><i class="fa fa-check"></i><b>15.4.2</b> Paired t-test</a></li>
<li class="chapter" data-level="15.4.3" data-path="comparing-means.html"><a href="comparing-means.html#the-paired-t-test-as-a-linear-model"><i class="fa fa-check"></i><b>15.4.3</b> The paired t-test as a linear model</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="comparing-means.html"><a href="comparing-means.html#comparing-more-than-two-means"><i class="fa fa-check"></i><b>15.5</b> Comparing more than two means</a><ul>
<li class="chapter" data-level="15.5.1" data-path="comparing-means.html"><a href="comparing-means.html#analysis-of-variance"><i class="fa fa-check"></i><b>15.5.1</b> Analysis of variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="practical-example.html"><a href="practical-example.html"><i class="fa fa-check"></i><b>16</b> The process of statistical modeling: A practical example</a><ul>
<li class="chapter" data-level="16.1" data-path="practical-example.html"><a href="practical-example.html#the-process-of-statistical-modeling"><i class="fa fa-check"></i><b>16.1</b> The process of statistical modeling</a><ul>
<li class="chapter" data-level="16.1.1" data-path="practical-example.html"><a href="practical-example.html#specify-your-question-of-interest"><i class="fa fa-check"></i><b>16.1.1</b> 1: Specify your question of interest</a></li>
<li class="chapter" data-level="16.1.2" data-path="practical-example.html"><a href="practical-example.html#identify-or-collect-the-appropriate-data"><i class="fa fa-check"></i><b>16.1.2</b> 2: Identify or collect the appropriate data</a></li>
<li class="chapter" data-level="16.1.3" data-path="practical-example.html"><a href="practical-example.html#prepare-the-data-for-analysis"><i class="fa fa-check"></i><b>16.1.3</b> 3: Prepare the data for analysis</a></li>
<li class="chapter" data-level="16.1.4" data-path="practical-example.html"><a href="practical-example.html#determine-the-appropriate-model"><i class="fa fa-check"></i><b>16.1.4</b> 4. Determine the appropriate model</a></li>
<li class="chapter" data-level="16.1.5" data-path="practical-example.html"><a href="practical-example.html#fit-the-model-to-the-data"><i class="fa fa-check"></i><b>16.1.5</b> 5. Fit the model to the data</a></li>
<li class="chapter" data-level="16.1.6" data-path="practical-example.html"><a href="practical-example.html#criticize-the-model-to-make-sure-it-fits-properly"><i class="fa fa-check"></i><b>16.1.6</b> 6. Criticize the model to make sure it fits properly</a></li>
<li class="chapter" data-level="16.1.7" data-path="practical-example.html"><a href="practical-example.html#test-hypothesis-and-quantify-effect-size"><i class="fa fa-check"></i><b>16.1.7</b> 7. Test hypothesis and quantify effect size</a></li>
<li class="chapter" data-level="16.1.8" data-path="practical-example.html"><a href="practical-example.html#what-about-possible-confounds"><i class="fa fa-check"></i><b>16.1.8</b> What about possible confounds?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html"><i class="fa fa-check"></i><b>17</b> Doing reproducible research</a><ul>
<li class="chapter" data-level="17.1" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#how-we-think-science-should-work"><i class="fa fa-check"></i><b>17.1</b> How we think science should work</a></li>
<li class="chapter" data-level="17.2" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#how-science-sometimes-actually-works"><i class="fa fa-check"></i><b>17.2</b> How science (sometimes) actually works</a></li>
<li class="chapter" data-level="17.3" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#the-reproducibility-crisis-in-science"><i class="fa fa-check"></i><b>17.3</b> The reproducibility crisis in science</a><ul>
<li class="chapter" data-level="17.3.1" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#positive-predictive-value-and-statistical-significance"><i class="fa fa-check"></i><b>17.3.1</b> Positive predictive value and statistical significance</a></li>
<li class="chapter" data-level="17.3.2" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#the-winners-curse"><i class="fa fa-check"></i><b>17.3.2</b> The winner’s curse</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#questionable-research-practices"><i class="fa fa-check"></i><b>17.4</b> Questionable research practices</a><ul>
<li class="chapter" data-level="17.4.1" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#esp-or-qrp"><i class="fa fa-check"></i><b>17.4.1</b> ESP or QRP?</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#doing-reproducible-research-1"><i class="fa fa-check"></i><b>17.5</b> Doing reproducible research</a><ul>
<li class="chapter" data-level="17.5.1" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#pre-registration"><i class="fa fa-check"></i><b>17.5.1</b> Pre-registration</a></li>
<li class="chapter" data-level="17.5.2" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#reproducible-practices"><i class="fa fa-check"></i><b>17.5.2</b> Reproducible practices</a></li>
<li class="chapter" data-level="17.5.3" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#replication"><i class="fa fa-check"></i><b>17.5.3</b> Replication</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#doing-reproducible-data-analysis"><i class="fa fa-check"></i><b>17.6</b> Doing reproducible data analysis</a></li>
<li class="chapter" data-level="17.7" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#conclusion-doing-better-science"><i class="fa fa-check"></i><b>17.7</b> Conclusion: Doing better science</a></li>
<li class="chapter" data-level="17.8" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#suggested-readings-11"><i class="fa fa-check"></i><b>17.8</b> Suggested Readings</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Thinking for the 21st Century</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-testing" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Hypothesis testing</h1>
<p>In the first chapter we discussed the three major goals of statistics:</p>
<ul>
<li>Describe</li>
<li>Decide</li>
<li>Predict</li>
</ul>
<p>In this chapter we will introduce the ideas behind the use of statistics to make decisions – in particular, decisions about whether a particular hypothesis is supported by the data.</p>
<div id="null-hypothesis-statistical-testing-nhst" class="section level2">
<h2><span class="header-section-number">9.1</span> Null Hypothesis Statistical Testing (NHST)</h2>
<p>The specific type of hypothesis testing that we will discuss is known (for reasons that will become clear) as <em>null hypothesis statistical testing</em> (NHST). If you pick up almost any scientific or biomedical research publication, you will see NHST being used to test hypotheses, and in their introductory psycholology textbook, Gerrig &amp; Zimbardo (2002) referred to NHST as the “backbone of psychological research”. Thus, learning how to use and interpret the results from hypothesis testing is essential to understand the results from this research.</p>
<p>It is also important for you to know, however, that NHST is deeply flawed, and that many statisticians and researchers (including myself) think that it has been the cause of serious problems in science, which we will discuss in Chapter <a href="doing-reproducible-research.html#doing-reproducible-research">17</a>. For more than 50 years, there have been calls to abandon NHST in favor of other approaches (like those that we will discuss in the following chapters):</p>
<ul>
<li>“The test of statistical significance in psychological research may be taken as an instance of a kind of essential mindlessness in the conduct of research” (Bakan, 1966)</li>
<li>Hypothesis testing is “a wrongheaded view about what constitutes scientific progress” (Luce, 1988)</li>
</ul>
<p>NHST is also widely misunderstood, largely because it violates our intuitions about how statistical hypothesis testing should work. Let’s look at an example to see.</p>
</div>
<div id="null-hypothesis-statistical-testing-an-example" class="section level2">
<h2><span class="header-section-number">9.2</span> Null hypothesis statistical testing: An example</h2>
<p>There is great interest in the use of body-worn cameras by police officers, which are thought to reduce the use of force and improve officer behavior. However, in order to establish this we need experimental evidence, and it has become increasingly common for governments to use randomized controlled trials to test such ideas. A randomized controlled trial of the effectiveness of body-worn cameras was performed by the Washington, DC government and DC Metropolitan Police Department in 2015/2016 in order to test the hypothesis that body-worn cameras are effective. Officers were randomly assigned to wear a body-worn camera or not, and their behavior was then tracked over time to determine whether the cameras resulted in less use of force and fewer civilian complaints about officer behavior.</p>
<p>Before we get to the results, let’s ask how you would think the statistical analysis might work. Let’s say we want to specifically test the hypothesis of whether the use of force is decreased by the wearing of cameras. The randomized controlled trial provides us with the data to test the hypothesis – namely, the rates of use of force by officers assigned to either the camera or control groups. The next obvious step is to look at the data and determine whether they provide convincing evidence for or against this hypothesis. That is: What is the likelihood that body-worn cameras reduce the use of force, given the data and everything else we know?</p>
<p>It turns out that this is <em>not</em> how null hypothesis testing works. Instead, we first take our hypothesis of interest (i.e. whether body-worn cameras reduce use of force), and flip it on its head, creating a <em>null hypothesis</em> – in this case, the null hypothesis would be that cameras do not reduce use of force. Importantly, we then assume that the null hypothesis is true. We then look at the data, and determine whether the data are sufficiently unlikely under the null hypothesis that we can reject the null in favor of the <em>alternative hypothesis</em> which is our hypothesis of interest. If there is not sufficient evidence to reject the null, then we say that we “failed to reject” the null.</p>
<p>Understanding some of the concepts of NHST, particularly the notorious “p-value”, is invariably challenging the first time one encounters them, because they are so counter-intuitive. As we will see later, there are other approaches that provide a much more intuitive way to address hypothesis testing (but have their own complexities). However, before we get to those, it’s important for you to have a deep understanding of how hypothesis testing works, because it’s clearly not going to go away any time soon.</p>
</div>
<div id="the-process-of-null-hypothesis-testing" class="section level2">
<h2><span class="header-section-number">9.3</span> The process of null hypothesis testing</h2>
<p>We can break the process of null hypothesis testing down into a number of steps:</p>
<ol style="list-style-type: decimal">
<li>Formulate a hypothesis that embodies our prediction (<em>before seeing the data</em>)</li>
<li>Collect some data relevant to the hypothesis</li>
<li>Specify null and alternative hypotheses</li>
<li>Fit a model to the data that represents the alternative hypothesis and compute a test statistic</li>
<li>Compute the probability of the observed value of that statistic assuming that the null hypothesis is true</li>
<li>Assess the “statistical significance” of the result</li>
</ol>
<p>For a hands-on example, let’s use the NHANES data to ask the following question: Is physical activity related to body mass index? In the NHANES dataset, participants were asked whether they engage regularly in moderate or vigorous-intensity sports, fitness or recreational activities (stored in the variable <span class="math inline">\(PhysActive\)</span>). They also measured height and weight and computed Body Mass Index:</p>
<p><span class="math display">\[
BMI = \frac{weight(kg)}{height(m)^2}
\]</span></p>
<div id="step-1-formulate-a-hypothesis" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Step 1: Formulate a hypothesis</h3>
<p>For step 1, we hypothesize that BMI should be greater for people who do not engage in physical activity, compared to those who do.</p>
</div>
<div id="step-2-collect-some-data" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Step 2: Collect some data</h3>
<p>For step 2, we collect some data. In this case, we will sample 250 individuals from the NHANES dataset. Figure <a href="hypothesis-testing.html#fig:bmiSample">9.1</a> shows an example of such a sample, with BMI shown separately for active and inactive individuals.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># sample 250 adults from NHANES and compute mean BMI separately for active</span>
<span class="co"># and inactive individuals</span>

sampSize &lt;-<span class="st"> </span><span class="dv">250</span>

NHANES_sample &lt;-<span class="st"> </span>
<span class="st">  </span>NHANES_adult <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sample_n</span>(sampSize)

sampleSummary &lt;-
<span class="st">  </span>NHANES_sample <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(PhysActive) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(
    <span class="dt">N =</span> <span class="kw">length</span>(BMI),
    <span class="dt">mean =</span> <span class="kw">mean</span>(BMI),
    <span class="dt">sd =</span> <span class="kw">sd</span>(BMI)
  )

<span class="co"># calculate the mean difference in BMI between active </span>
<span class="co"># and inactive individuals; we&#39;ll use this later to calculate the t-statistic</span>
meanDiff &lt;-<span class="st"> </span>
<span class="st">  </span>sampleSummary <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(
    PhysActive,
    mean
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(PhysActive, mean) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">meanDiff =</span> No <span class="op">-</span><span class="st"> </span>Yes
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(meanDiff)

<span class="co"># calculate the summed variances in BMI for active </span>
<span class="co"># and inactive individuals; we&#39;ll use this later to calculate the t-statistic</span>
sumVariance &lt;-<span class="st"> </span>
<span class="st">  </span>sampleSummary <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(
    PhysActive,
    N,
    sd
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(column, stat, N<span class="op">:</span>sd) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">unite</span>(temp, PhysActive, column) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">spread</span>(temp, stat) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">sumVariance =</span> No_sd<span class="op">**</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>No_N <span class="op">+</span><span class="st"> </span>Yes_sd<span class="op">**</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>Yes_N
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(sumVariance)


<span class="co"># print sampleSummary table</span>
<span class="kw">pander</span>(sampleSummary)</code></pre></div>
<table style="width:49%;">
<colgroup>
<col width="18%" />
<col width="8%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">PhysActive</th>
<th align="center">N</th>
<th align="center">mean</th>
<th align="center">sd</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">No</td>
<td align="center">135</td>
<td align="center">30.25</td>
<td align="center">8.2</td>
</tr>
<tr class="even">
<td align="center">Yes</td>
<td align="center">115</td>
<td align="center">28.6</td>
<td align="center">6.88</td>
</tr>
</tbody>
</table>
<div class="figure"><span id="fig:bmiSample"></span>
<img src="StatsThinking21_files/figure-html/bmiSample-1.png" alt="Box plot of BMI data from a sample of adults from the NHANES dataset, split by whether they reported engaging in regular physical activity." width="384" height="50%" />
<p class="caption">
Figure 9.1: Box plot of BMI data from a sample of adults from the NHANES dataset, split by whether they reported engaging in regular physical activity.
</p>
</div>
</div>
<div id="step-3-specify-the-null-and-alternative-hypotheses" class="section level3">
<h3><span class="header-section-number">9.3.3</span> Step 3: Specify the null and alternative hypotheses</h3>
<p>For step 3, we need to specify our null hypothesis (which we call <span class="math inline">\(H_0\)</span>) and our alternative hypothesis (which we call <span class="math inline">\(H_A\)</span>). <span class="math inline">\(H_0\)</span> is the baseline against which we test our hypothesis of interest: that is, what would we expect the data to look like if there was no effect? The null hypothesis always involves some kind of equality (=, <span class="math inline">\(\le\)</span>, or <span class="math inline">\(\ge\)</span>). <span class="math inline">\(H_A\)</span> describes what we expect if there actually is an effect. The alternative hypothesis always involves some kind of inequality (<span class="math inline">\(\ne\)</span>, &gt;, or &lt;). Importantly, null hypothesis testing operates under the assumption that the null hypothesis is true unless the evidence shows otherwise.</p>
<p>We also have to decide whether to use <em>directional</em> or <em>non-directional</em> hypotheses. A non-directional hypothesis simply predicts that there will be a difference, without predicting which direction it will go. For the BMI/activity example, a non-directional null hypothesis would be:</p>
<p><span class="math inline">\(H0: BMI_{active} = BMI_{inactive}\)</span></p>
<p>and the corresponding non-directional alternative hypothesis would be:</p>
<p><span class="math inline">\(HA: BMI_{active} \neq BMI_{inactive}\)</span></p>
<p>A directional hypothesis, on the other hand, predicts which direction the difference would go. For example, we have strong prior knowledge to predict that people who engage in physical activity should weigh less than those who do not, so we would propose the following directional null hypothesis:</p>
<p><span class="math inline">\(H0: BMI_{active} \ge BMI_{inactive}\)</span></p>
<p>and directional alternative:</p>
<p><span class="math inline">\(H0: BMI_{active} &lt; BMI_{inactive}\)</span></p>
</div>
<div id="step-4-fit-a-model-to-the-data-and-compute-a-test-statistic" class="section level3">
<h3><span class="header-section-number">9.3.4</span> Step 4: Fit a model to the data and compute a test statistic</h3>
<p>For step 4, we want to use the data to compute a statistic that will ultimately let us decide whether the null hypothesis is rejected or not. To do this, the model needs to quantify the amount of evidence in favor of the alternative hypothesis, relative to the variability in the data. Thus we can think of the test statistic as providing a measure of the size of the effect compared to the variability in the data. In general, this test statistic will have a probability distribution associated with it, because that allows us to determine how likely our observed value of the statistic is under the null hypothesis.</p>
<p>For the BMI example, we need a test statistic that allows us to test for a difference between two means, since the hypotheses are stated in terms of mean BMI for each group. One statistic that is often used to compare two means is the <em>t-statistic</em>, first developed by the statistician William Sealy Gossett, who worked for the Guiness Brewery in Dublin and wrote under the pen name “Student” - hence, it is often called “Student’s t-statistic”. The t-statistic is appropriate for comparing the means of two groups when the sample sizes are relatively small and the population standard deviation is unknown. The t-statistic for comparison of two independent groups is computed as:</p>
<p><span class="math display">\[
t = \frac{\bar{X_1} - \bar{X_2}}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}
\]</span></p>
<p>where <span class="math inline">\(\bar{X}_1\)</span> and <span class="math inline">\(\bar{X}_2\)</span> are the means of the two groups, <span class="math inline">\(S^2_1\)</span> and <span class="math inline">\(S^2_2\)</span> are the estimated variances of the groups, and <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> are the sizes of the two groups. The t-statistic is distributed according to a probability distribution known as a <em>t</em> distribution. The <em>t</em> distribution looks quite similar to a normal distribution, but it differs depending on the number of degrees of freedom, which for this example is the number of observations minus 2, since we have computed two means and thus given up two degrees of freedom. When the degrees of freedom are large (say 1000), then the <em>t</em> distribution looks essentialy like the normal distribution, but when they are small then the <em>t</em> distribution has longer tails than the normal (see Figure <a href="hypothesis-testing.html#fig:tVersusNormal">9.2</a>).</p>
<div class="figure"><span id="fig:tVersusNormal"></span>
<img src="StatsThinking21_files/figure-html/tVersusNormal-1.png" alt="Each panel shows the t distribution (in blue dashed line) overlaid on the normal distribution (in solid red line).  The left panel shows a t distribution with 4 degrees of freedom, in which case the distribution is similar but has slightly wider tails.  The right panel shows a t distribution with 1000 degrees of freedom, in which case it is virtually identical to the normal." width="384" height="50%" />
<p class="caption">
Figure 9.2: Each panel shows the t distribution (in blue dashed line) overlaid on the normal distribution (in solid red line). The left panel shows a t distribution with 4 degrees of freedom, in which case the distribution is similar but has slightly wider tails. The right panel shows a t distribution with 1000 degrees of freedom, in which case it is virtually identical to the normal.
</p>
</div>
</div>
<div id="step-5-determine-the-probability-of-the-data-under-the-null-hypothesis" class="section level3">
<h3><span class="header-section-number">9.3.5</span> Step 5: Determine the probability of the data under the null hypothesis</h3>
<p>This is the step where NHST starts to violate our intuition – rather than determining the likelihood that the null hypothesis is true given the data, we instead determine the likelihood of the data under the null hypothesis - because we started out by assuming that the null hypothesis is true! To do this, we need to know the probability distribution for the statistic under the null hypothesis, so that we can ask how likely the data are under that distribution. Before we move to our BMI data, let’s start with some simpler examples.</p>
<div id="randomization-a-very-simple-example" class="section level5">
<h5><span class="header-section-number">9.3.5.0.1</span> Randomization: A very simple example</h5>
<p>Let’s say that we wish to determine whether a coin is fair. To collect data, we flip the coin 100 times, and we count 70 heads. In this example, <span class="math inline">\(H_0: P(heads)=0.5\)</span> and <span class="math inline">\(H_A: P(heads) \neq 0.5\)</span>, and our test statistic is simply the number of heads that we counted. The question that we then want to as is: How likely is it that we would observe 70 heads if the true probability of heads is 0.5. We can imagine that this might happen very occasionally just by chance, but doesn’t seem very likely. To quantify this probability, we can use the <em>binomial distribution</em>:</p>
<p><span class="math display">\[
P(X &lt; k) = \sum_{i=0}^k \binom{N}{k} p^i (1-p)^{(n-i)}
\]</span> This equation will tell us the likelihood of a certain number of heads or fewer, given a particular probability of heads. However, what we really want to know is the probability of a certain number or more, which we can obtain by subtracting from one:</p>
<p><span class="math display">\[
P(X \ge k) = 1 - P(X &lt; k)
\]</span></p>
<p>We can compute the probability for our example using the <code>pbinom()</code> function in R as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the probability of 69 or fewer heads, when P(heads)=0.5</span>
p_lt_<span class="dv">70</span> &lt;-<span class="st"> </span><span class="kw">pbinom</span>(<span class="dv">69</span>, <span class="dv">100</span>, <span class="fl">0.5</span>) 
<span class="kw">sprintf</span>(<span class="st">&quot;probability of 69 or fewer heads given P(heads)=0.5: %0.6f&quot;</span>, p_lt_<span class="dv">70</span>)</code></pre></div>
<pre><code>## [1] &quot;probability of 69 or fewer heads given P(heads)=0.5: 0.999961&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># the probability of 70 or more heads is simply the complement of p_lt_70</span>
p_ge_<span class="dv">70</span> &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_lt_<span class="dv">70</span>
<span class="kw">sprintf</span>(<span class="st">&quot;probability of 70 or more heads given P(heads)=0.5: %0.6f&quot;</span>, p_ge_<span class="dv">70</span>)</code></pre></div>
<pre><code>## [1] &quot;probability of 70 or more heads given P(heads)=0.5: 0.000039&quot;</code></pre>
<p>This computation shows us that the likelihood of getting 70 heads if the coin is indeed fair is very small. Now, what if we didn’t have the <code>pbinom()</code> function to tell us the probability of that number of heads? We could instead determine it by simulation – we repeatly flip a coin 100 times using a true probability of 0.5, and then compute the distribution of the number of heads across those simulation runs. Figure <a href="hypothesis-testing.html#fig:coinFlips">9.3</a> shows the result from this simulation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate tossing of 100,000 flips of 100 coins to identify empirical </span>
<span class="co"># probability of 70 or more heads out of 100 flips</span>

<span class="co"># create function to toss coins</span>
tossCoins &lt;-<span class="st"> </span><span class="cf">function</span>() {
  flips &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">100</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span> 
  <span class="kw">return</span>(<span class="kw">sum</span>(flips))
}

<span class="co"># use a large number of replications since this is fast</span>
coinFlips &lt;-<span class="st"> </span><span class="kw">replicate</span>(<span class="dv">100000</span>, <span class="kw">tossCoins</span>())

p_ge_70_sim &lt;-<span class="st"> </span><span class="kw">mean</span>(coinFlips <span class="op">&gt;=</span><span class="st"> </span><span class="dv">70</span>)
<span class="kw">sprintf</span>(
  <span class="st">&quot;empirical probability of 70 or more heads given P(heads)=0.5: %0.6f&quot;</span>, 
  p_ge_70_sim
)</code></pre></div>
<pre><code>## [1] &quot;empirical probability of 70 or more heads given P(heads)=0.5: 0.000020&quot;</code></pre>
<div class="figure"><span id="fig:coinFlips"></span>
<img src="StatsThinking21_files/figure-html/coinFlips-1.png" alt="Distribution of numbers of heads (out of 100 flips) across 100,000 simulated runs." width="384" height="50%" />
<p class="caption">
Figure 9.3: Distribution of numbers of heads (out of 100 flips) across 100,000 simulated runs.
</p>
</div>
<p>Here we can see that the probability computed via simulation (0.000020) is very close to the theoretical probability (.00004).</p>
<p>Let’s do the analogous computation for our BMI example. First we compute the t statistic using the values from our sample that we calculated above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tStat &lt;-<span class="st"> </span>
<span class="st">  </span>meanDiff <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(sumVariance)

<span class="kw">sprintf</span>(<span class="st">&quot;t statistic = %0.3f&quot;</span>, tStat)</code></pre></div>
<pre><code>## [1] &quot;t statistic = 1.735&quot;</code></pre>
<p>The question that we then want to ask is: What is the likelihood that we would find a t statistic of this size, if the true difference between groups is zero or less (i.e. the directional null hypothesis)?<br />
We can use the t distribution to determine this probability. Our sample size is 250, so the appropriate t distribution has 248 degrees of freedom. We can use the <code>pt()</code> function in R to determine the probability of finding a value of the t-statistic greater than or equal to our observed value. Note that we want to know the probability of a value greater than our observed value, but by default <code>pt()</code> gives us the probability of a value less than the one that we provide it, so we have to tell it explicitly to provide us with the “upper tail” probability (by setting <code>lower.tail = FALSE</code>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pvalue_tdist &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">pt</span>(tStat, <span class="dt">df =</span> <span class="dv">248</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)

<span class="kw">sprintf</span>(<span class="st">&quot;p(t &gt; %0.2f, df = 248) = %0.3f&quot;</span>, tStat, pvalue_tdist)</code></pre></div>
<pre><code>## [1] &quot;p(t &gt; 1.74, df = 248) = 0.042&quot;</code></pre>
<p>This tells us that our observed t-statistic value of 1.74 is relatively unlikely if the null hypothesis really is true.</p>
<p>In this case, we used a directional hypothesis, so we only had to look at one end of the null distribution. If we wanted to test a non-directional hypothesis, then we would need to be able to identify how unexpected the size of the effect is, regardless of its direction. In the context of the t-test, this means that we need to know how likely it is that the statistic would be as extreme in either the positive or negative direction. To do this, we multiply the observed <em>t</em> value by -1, since the <em>t</em> distribution is centered around zero, and then add together the two tail probabilities to get a <em>two-tailed</em> p-value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pvalue_tdist_twotailed &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">pt</span>(tStat, <span class="dt">df =</span> <span class="dv">248</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">pt</span>(<span class="op">-</span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span>tStat, <span class="dt">df =</span> <span class="dv">248</span>, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>)

<span class="kw">sprintf</span>(
  <span class="st">&quot;p(t &gt; %0.2f or t&lt; %0.2f, df = 248) = %0.3f&quot;</span>, 
  tStat, 
  <span class="op">-</span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span>tStat, pvalue_tdist_twotailed
)</code></pre></div>
<pre><code>## [1] &quot;p(t &gt; 1.74 or t&lt; -1.74, df = 248) = 0.084&quot;</code></pre>
<p>Here we see that the p value for the two-tailed test is twice as large as that for the one-tailed test, which reflects the fact that an extreme value is less surprising since it could have occurred in either direction.</p>
<p>How do you choose whether to use a one-tailed versus a two-tailed test? The two-tailed test is always going to be more conservative, so it’s always a good bet to use that one, unless you had a very strong prior reason for using a one-tailed test. In that case, you should have written down the hypothesis before you ever looked at the data. In Chapter (doing-reproducible-research) we will discuss the idea of pre-registration of hypotheses, which formalizes the idea of writing down your hypotheses before you ever see the actual data. You should <em>never</em> make a decision about how to perform a hypothesis test once you have looked at the data, as this can introduce serious bias into the results.</p>
</div>
<div id="computing-p-values-using-randomization" class="section level4">
<h4><span class="header-section-number">9.3.5.1</span> Computing p-values using randomization</h4>
<p>So far we have seen how we can use the t-distribution to compute the probability of the data under the null hypothesis, but we can also do this using simulation. The basic idea is that we generate simulated data like those that we would expect under the null hypothesis, and then ask how extreme the observed data are in comparison to those simulated data. The key question is: How can we generate data for which the null hypothesis is true? The general answer is that we can randomly rearrange the data in a specific way that makes the data look like they would if the null was really true. This is similar to the idea of bootstrapping, in the sense that it uses our own data to come up with an answer, but it does it in a different way.</p>
<div id="randomization-a-simple-example" class="section level5">
<h5><span class="header-section-number">9.3.5.1.1</span> Randomization: a simple example</h5>
<p>Let’s start with a simple example. Let’s say that we want to compare the mean squatting ability of football players with cross-country runners, with <span class="math inline">\(H_0: \mu_{FB} \le \mu_{XC}\)</span> and <span class="math inline">\(H_A: \mu_{FB} &gt; \mu_{XC}\)</span>. We measure the maximum squatting ability of 5 football players and 5 cross-country runners (which we will generate randomly, assuming that <span class="math inline">\(\mu_{FB} = 300\)</span>, <span class="math inline">\(\mu_{XC} = 140\)</span>, and <span class="math inline">\(\sigma = 30\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># generate simulated data for squatting ability across football players </span>
<span class="co"># and cross country runners</span>

<span class="co"># reset random seed for this example</span>
<span class="kw">set.seed</span>(<span class="dv">12345678</span>)

<span class="co"># create a function to round values to nearest product of 5,</span>
<span class="co"># to keep example simple</span>
roundToNearest5 &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">base =</span> <span class="dv">5</span>) {
  <span class="kw">return</span>(base <span class="op">*</span><span class="st"> </span><span class="kw">round</span>(x <span class="op">/</span><span class="st"> </span>base))
}

<span class="co"># create and show data frame containing simulated data</span>
squatDf &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">group =</span> <span class="kw">as.factor</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;FB&quot;</span>, <span class="dv">5</span>), <span class="kw">rep</span>(<span class="st">&quot;XC&quot;</span>, <span class="dv">5</span>))),
  <span class="dt">squat =</span> <span class="kw">roundToNearest5</span>(<span class="kw">c</span>(<span class="kw">rnorm</span>(<span class="dv">5</span>) <span class="op">*</span><span class="st"> </span><span class="dv">30</span> <span class="op">+</span><span class="st"> </span><span class="dv">300</span>, <span class="kw">rnorm</span>(<span class="dv">5</span>) <span class="op">*</span><span class="st"> </span><span class="dv">30</span> <span class="op">+</span><span class="st"> </span><span class="dv">140</span>))
)

<span class="kw">pander</span>(squatDf)</code></pre></div>
<table style="width:22%;">
<colgroup>
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">group</th>
<th align="center">squat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">FB</td>
<td align="center">335</td>
</tr>
<tr class="even">
<td align="center">FB</td>
<td align="center">350</td>
</tr>
<tr class="odd">
<td align="center">FB</td>
<td align="center">230</td>
</tr>
<tr class="even">
<td align="center">FB</td>
<td align="center">290</td>
</tr>
<tr class="odd">
<td align="center">FB</td>
<td align="center">325</td>
</tr>
<tr class="even">
<td align="center">XC</td>
<td align="center">115</td>
</tr>
<tr class="odd">
<td align="center">XC</td>
<td align="center">115</td>
</tr>
<tr class="even">
<td align="center">XC</td>
<td align="center">170</td>
</tr>
<tr class="odd">
<td align="center">XC</td>
<td align="center">175</td>
</tr>
<tr class="even">
<td align="center">XC</td>
<td align="center">215</td>
</tr>
</tbody>
</table>
<div class="figure"><span id="fig:squatPlot"></span>
<img src="StatsThinking21_files/figure-html/squatPlot-1.png" alt="Box plots of simulated squatting ability for football players and cross-country runners." width="384" height="50%" />
<p class="caption">
Figure 9.4: Box plots of simulated squatting ability for football players and cross-country runners.
</p>
</div>
<p>From the plot in Figure <a href="hypothesis-testing.html#fig:squatPlot">9.4</a> it’s clear that there is a large difference between the two groups. We can do a standard t-test to test our hypothesis, using the <code>t.test()</code> command in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute and print t statistic comparing two groups</span>

tt &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">t.test</span>(
    squat <span class="op">~</span><span class="st"> </span>group, 
    <span class="dt">data =</span> squatDf, 
    <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>, 
    <span class="dt">var.equal =</span> <span class="ot">TRUE</span>
  )

<span class="kw">sprintf</span>(<span class="st">&quot;p(t &gt; %0.2f, df = 8) = %0.5f&quot;</span>, tt<span class="op">$</span>statistic, tt<span class="op">$</span>p.value)</code></pre></div>
<pre><code>## [1] &quot;p(t &gt; 5.14, df = 8) = 0.00044&quot;</code></pre>
<p>This shows that the likelihood of such a difference under the null hypothesis is very small, using the <em>t</em> distribution to define the null. Now let’s see how we could answer the same question using randomization. The basic idea is that if the null hypothesis of no difference between groups is true, then it shouldn’t matter which group one comes from (football players versus cross-country runners) – thus, to create data that are like our actual data but also conform to the null hypothesis, we can randomly reorder the group labels for the individuals in the dataset, and then recompute the difference between the groups. The results of such a shuffle are shown in Figure <a href="hypothesis-testing.html#fig:scramPlot">9.5</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a scrambled version of the group membership variable</span>

dfScram &lt;-
<span class="st">  </span>squatDf <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">scrambledGroup =</span> <span class="kw">sample</span>(group)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>group)

<span class="kw">pander</span>(dfScram)</code></pre></div>
<table style="width:33%;">
<colgroup>
<col width="11%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">squat</th>
<th align="center">scrambledGroup</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">335</td>
<td align="center">FB</td>
</tr>
<tr class="even">
<td align="center">350</td>
<td align="center">XC</td>
</tr>
<tr class="odd">
<td align="center">230</td>
<td align="center">FB</td>
</tr>
<tr class="even">
<td align="center">290</td>
<td align="center">XC</td>
</tr>
<tr class="odd">
<td align="center">325</td>
<td align="center">XC</td>
</tr>
<tr class="even">
<td align="center">115</td>
<td align="center">FB</td>
</tr>
<tr class="odd">
<td align="center">115</td>
<td align="center">FB</td>
</tr>
<tr class="even">
<td align="center">170</td>
<td align="center">XC</td>
</tr>
<tr class="odd">
<td align="center">175</td>
<td align="center">FB</td>
</tr>
<tr class="even">
<td align="center">215</td>
<td align="center">XC</td>
</tr>
</tbody>
</table>
<div class="figure"><span id="fig:scramPlot"></span>
<img src="StatsThinking21_files/figure-html/scramPlot-1.png" alt="Box plots for subjects assigned to each group after scrambling group labels." width="384" height="50%" />
<p class="caption">
Figure 9.5: Box plots for subjects assigned to each group after scrambling group labels.
</p>
</div>
<p>After scrambling the labels, we see that the two groups are now much more similar, and in fact the cross-country group now has a slightly higher mean. Now let’s do that 10000 times and store the t statistic for each iteration; this may take a moment to complete.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># shuffle data 10,000 times and compute distribution of t values</span>

nRuns &lt;-<span class="st"> </span><span class="dv">10000</span>

shuffleAndMeasure &lt;-<span class="st"> </span><span class="cf">function</span>(df) {
  dfScram &lt;-<span class="st"> </span>
<span class="st">    </span>df <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(
      <span class="dt">scrambledGroup =</span> <span class="kw">sample</span>(group)
    )
  tt &lt;-<span class="st"> </span><span class="kw">t.test</span>(
    squat <span class="op">~</span><span class="st"> </span>scrambledGroup,
    <span class="dt">data =</span> dfScram,
    <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>, 
    <span class="dt">var.equal =</span> <span class="ot">TRUE</span>
  )
  <span class="kw">return</span>(tt<span class="op">$</span>statistic)
}

shuffleDiff &lt;-<span class="st"> </span><span class="kw">replicate</span>(nRuns, <span class="kw">shuffleAndMeasure</span>(squatDf))

<span class="kw">sprintf</span>(<span class="st">&quot;mean t value across shuffles = %0.3f&quot;</span>, <span class="kw">mean</span>(shuffleDiff))</code></pre></div>
<pre><code>## [1] &quot;mean t value across shuffles = -0.004&quot;</code></pre>
<p>We can now look at the distribution of mean differences across the shuffled datasets. Figure <a href="hypothesis-testing.html#fig:shuffleHist">9.6</a> shows the histogram of the group differences across all of the random shuffles. As expected under the null hypothesis, this distribution is centered at zero.</p>
<div class="figure"><span id="fig:shuffleHist"></span>
<img src="StatsThinking21_files/figure-html/shuffleHist-1.png" alt="Histogram of differences between the football and cross-country groups after randomly shuffling group membership.  The red line denotes the actual difference observed between the two groups, and the blue line shows the theoretical t distribution for this analysis." width="384" height="50%" />
<p class="caption">
Figure 9.6: Histogram of differences between the football and cross-country groups after randomly shuffling group membership. The red line denotes the actual difference observed between the two groups, and the blue line shows the theoretical t distribution for this analysis.
</p>
</div>
<p>We can see that the distribution of t values after shuffling roughly follows the theoretical t distribution under the null hypothesis (with mean=0), showing that randomization worked to generate null data. We also see something interesting if we compare the shuffled t values to the actual t value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute number of runs on which t statistic for shuffle data was </span>
<span class="co"># equal to observed t statistic</span>

equalSum &lt;-<span class="st"> </span><span class="kw">sum</span>(shuffleDiff <span class="op">==</span><span class="st"> </span>tt<span class="op">$</span>statistic)
<span class="kw">sprintf</span>(<span class="st">&quot;Number of runs on which shuffled t == observed t: %d&quot;</span>, equalSum)</code></pre></div>
<pre><code>## [1] &quot;Number of runs on which shuffled t == observed t: 33&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute number of runs on which t statistic for shuffle data was</span>
<span class="co"># equal to observed t statistic times -1</span>

equalSumMinus &lt;-<span class="st"> </span><span class="kw">sum</span>(shuffleDiff <span class="op">==</span><span class="st"> </span>tt<span class="op">$</span>statistic <span class="op">*</span><span class="st"> </span><span class="op">-</span><span class="dv">1</span>)
<span class="kw">sprintf</span>(<span class="st">&quot;Number of runs on which shuffled t == observed t*-1: %d&quot;</span>, equalSumMinus)</code></pre></div>
<pre><code>## [1] &quot;Number of runs on which shuffled t == observed t*-1: 28&quot;</code></pre>
<p>There are 33 shuffle runs on which the t statistic for the shuffled data was exactly the same as the observed data – which means that shuffling resulted in the same labeling as the actual data! This is unlikely, but not <em>that</em> unlikely, and we can actually compute its likelihood using a bit of probability theory. The number of possible permutations of 10 items is <span class="math inline">\(10!\)</span> which comes out to 3628800. The number of possible rearrangements of each set of 5 is <span class="math inline">\(5!\)</span> which comes out to 120, so the number of possible rearrangements of two sets of five is <span class="math inline">\(5! * 5!\)</span> or 14400. Thus, we expect that 0.0039 of the random labelings should come out exactly the same as the original, which is fairly close to the 0.0033 that we see in our simulation. We have a similar expectation for the number of times that the labeling will be exactly opposite of the true labeling, giving us the negative of the observed t value.</p>
<p>We can compute the p-value from the randomized data by measuring how many of the shuffled values are at least as extreme as the observed value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute p value using randomization</span>
pvalRandomization &lt;-<span class="st"> </span><span class="kw">mean</span>(shuffleDiff <span class="op">&gt;=</span><span class="st"> </span>tt<span class="op">$</span>statistic)

<span class="kw">sprintf</span>(
  <span class="st">&#39;p(t &gt; %0.2f, df = 8) using randomization = %0.5f&#39;</span>,
  tt<span class="op">$</span>statistic,
  pvalRandomization
)</code></pre></div>
<pre><code>## [1] &quot;p(t &gt; 5.14, df = 8) using randomization = 0.00330&quot;</code></pre>
<p>This p-value is very similar to the p-value that we obtained using the t distribution, and both are quite extreme, suggesting that the observed data are very unlikely to have arisen if the null hypothesis is true - and in this case we <em>know</em> that it’s not true, because we generated the data.</p>
</div>
<div id="randomization-bmiactivity-example" class="section level5">
<h5><span class="header-section-number">9.3.5.1.2</span> Randomization: BMI/activity example</h5>
<p>Now let’s use randomization to compute the p-value for the BMI/activity example. In this case, we will randomly shuffle the <code>PhysActive</code> variable and compute the difference between groups after each shuffle, and then compare our observed t statistic to the distribution of t statistics from the shuffled datasets.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create function to shuffle BMI data</span>

shuffleBMIstat &lt;-<span class="st"> </span><span class="cf">function</span>() {
  bmiDataShuffled &lt;-<span class="st"> </span>
<span class="st">    </span>NHANES_sample <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">select</span>(BMI, PhysActive) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(
      <span class="dt">PhysActive =</span> <span class="kw">sample</span>(PhysActive)
    )
  <span class="co"># compute the difference</span>
  simResult &lt;-<span class="st"> </span><span class="kw">t.test</span>(
    BMI <span class="op">~</span><span class="st"> </span>PhysActive,
    <span class="dt">data =</span> bmiDataShuffled,
    <span class="dt">var.equal =</span> <span class="ot">TRUE</span>
  )
  <span class="kw">return</span>(simResult<span class="op">$</span>statistic)
}

<span class="co"># run function 5000 times and save output</span>

nRuns &lt;-<span class="st"> </span><span class="dv">5000</span>
meanDiffSimDf &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">data.frame</span>(
    <span class="dt">meanDiffSim =</span> <span class="kw">replicate</span>(nRuns, <span class="kw">shuffleBMIstat</span>())
  )</code></pre></div>
<p>Let’s look at the results. Figure <a href="hypothesis-testing.html#fig:simDiff">9.7</a> shows the distribution of t values from the shuffled samples, and we can also compute the probability of finding a value as large or larger than the observed value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the empirical probability of t values larger than observed</span>
<span class="co"># value under the randomization null</span>
bmtTTest &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">t.test</span>(
  BMI <span class="op">~</span><span class="st"> </span>PhysActive,
  <span class="dt">data =</span> NHANES_sample,
  <span class="dt">var.equal =</span> <span class="ot">TRUE</span>, 
  <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>
)

bmiPvalRand &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">mean</span>(meanDiffSimDf<span class="op">$</span>meanDiffSim <span class="op">&gt;=</span><span class="st"> </span>bmtTTest<span class="op">$</span>statistic)

<span class="kw">sprintf</span>(
  <span class="st">&quot;p(mean &gt; %0.2f, df = 248) using randomization = %0.5f&quot;</span>, 
  bmtTTest<span class="op">$</span>statistic, 
  bmiPvalRand
)</code></pre></div>
<pre><code>## [1] &quot;p(mean &gt; 1.71, df = 248) using randomization = 0.04380&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sprintf</span>(
  <span class="st">&quot;p(mean &gt; %0.2f, df = 248) using parametric t-test = %0.5f&quot;</span>, 
  bmtTTest<span class="op">$</span>statistic, 
  bmtTTest<span class="op">$</span>p.value
  )</code></pre></div>
<pre><code>## [1] &quot;p(mean &gt; 1.71, df = 248) using parametric t-test = 0.04413&quot;</code></pre>
<div class="figure"><span id="fig:simDiff"></span>
<img src="StatsThinking21_files/figure-html/simDiff-1.png" alt="Histogram of t statistics after shuffling of group labels, with the observed value of the t statistic shown in the blue line, and values more extreme than the observed value shown in orange." width="384" height="50%" />
<p class="caption">
Figure 9.7: Histogram of t statistics after shuffling of group labels, with the observed value of the t statistic shown in the blue line, and values more extreme than the observed value shown in orange.
</p>
</div>
<p>Again, the p-value obtained from randomization (0.044) is very similar to the one obtained using the t distribution (0.044). The advantage of the randomization test is that it doesn’t require that we assume that the data from each of the groups are normally distributed, though the t-test is generally quite robust to violations of that assumption. In addition, the randomization test can allow us to compute p-values for statistics when we don’t have a theoretical distribution like we do for the t-test.</p>
<p>We do have to make one main assumption when we use the randomization test, which we refer to as <em>exchangeability</em>. This means that all of the observations are distributed in the same way, such that we can interchange them without changing the overall distribution. The main place where this can break down is when there are related observations in the data; for example, if we had data from individuals in 4 different families, then we couldn’t assume that individuals were exchangeable, because siblings would be closer to each other than they are to individuals from other families. In general, if the data were obtained by random sampling, then the assumption of exchangeability should hold.</p>
</div>
</div>
</div>
<div id="step-6-assess-the-statistical-significance-of-the-result" class="section level3">
<h3><span class="header-section-number">9.3.6</span> Step 6: Assess the “statistical significance” of the result</h3>
<p>The next step is to determine whether the p-value that results from the previous step is small enough that we are willing to reject the null hypothesis and conclude instead that the alternative is true. How much evidence do we require? This is one of the most controversial questions in statistics, in part because it requires a subjective judgment – there is no “correct” answer.</p>
<p>Historically, the most common answer to this question has been that we should reject the null hypothesis if the p-value is less than 0.05. This comes from the writings of Ronald Fisher, who has been referred to as “the single most important figure in 20th century statistics”<span class="citation">(Efron 1998)</span>:</p>
<blockquote>
<p>“If P is between .1 and .9 there is certainly no reason to suspect the hypothesis tested. If it is below .02 it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 … it is convenient to draw the line at about the level at which we can say: Either there is something in the treatment, or a coincidence has occurred such as does not occur more than once in twenty trials” <span class="citation">(Fisher 1925)</span></p>
</blockquote>
<p>However, Fisher never intended <span class="math inline">\(p &lt; 0.05\)</span> to be a fixed rule:</p>
<blockquote>
<p>“no scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas” [<a href="fish:1956" class="uri">fish:1956</a>]</p>
</blockquote>
<p>Instead, it is likely that it became a ritual due to the reliance upon tables of p-values that were used before computing made it easy to compute p values for arbitrary values of a statistic. All of the tables had an entry for 0.05, making it easy to determine whether one’s statistic exceeded the value needed to reach that level of significance.</p>
<p>The choice of statistical thresholds remains deeply controversial, and recently (Benjamin et al., 2018) it has been proposed that the standard threshold be changed from .05 to .005, making it substantially more stringent and thus more difficult to reject the null hypothesis. In large part this move is due to growing concerns that the evidence obtained from a significant result at <span class="math inline">\(p &lt; .05\)</span> is relatively weak; we will discuss this in much more detail in our later discussion of reproducibility in Chapter (doing-reproducible-research).</p>
<div id="hypothesis-testing-as-decision-making-the-neyman-pearson-approach" class="section level4">
<h4><span class="header-section-number">9.3.6.1</span> Hypothesis testing as decision-making: The Neyman-Pearson approach</h4>
<p>Whereas Fisher thought that the p-value could provide evidence regarding a specific hypothesis, the statisticians Jerzy Neyman and Egon Pearson disagreed vehemently. Instead, they proposed that we think of hypothesis testing in terms of its error rate in the long run:</p>
<blockquote>
<p>“no test based upon a theory of probability can by itself provide any valuable evidence of the truth or falsehood of a hypothesis. But we may look at the purpose of tests from another viewpoint. Without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behaviour with regard to them, in following which we insure that, in the long run of experience, we shall not often be wrong” <span class="citation">(J. Neyman and Pearson 1933)</span></p>
</blockquote>
<p>That is: We can’t know which specific decisions are right or wrong, but if we follow the rules, we can at least know how often our decisions will be wrong.</p>
<p>To understand the decision making framework that Neyman and Pearson developed, we first need to discuss statistical decision making in terms of the kinds of outcomes that can occur. There are two possible states of reality (<span class="math inline">\(H_0\)</span> is true, or <span class="math inline">\(H_0\)</span> is false), and two possible decisions (reject <span class="math inline">\(H_0\)</span>, or fail to reject <span class="math inline">\(H_0\)</span>). There are two ways in which we can make a correct decision:</p>
<ul>
<li>We can decide to reject <span class="math inline">\(H_0\)</span> when it is false (in the language of decision theory, we call this a <em>hit</em>)</li>
<li>We can fail to reject <span class="math inline">\(H_0\)</span> when it is true (we call this a <em>correct rejection</em>)</li>
</ul>
<p>There are also two kinds of errors we can make:</p>
<ul>
<li>We can decide to reject <span class="math inline">\(H_0\)</span> when it is actually true (we call this a <em>false alarm</em>, or <em>Type I error</em>)</li>
<li>We can fail to reject <span class="math inline">\(H_0\)</span> when it is actually false (we call this a <em>miss</em>, or <em>Type II error</em>)</li>
</ul>
<p>Neyman and Pearson coined two terms to describe the probability of these two types of errors in the long run:</p>
<ul>
<li>P(Type I error) = <span class="math inline">\(\alpha\)</span></li>
<li>P(Type II error) = <span class="math inline">\(\beta\)</span></li>
</ul>
<p>That is, if we set <span class="math inline">\(\alpha\)</span> to .05, then in the long run we should make a Type I error 5% of the time. Whereas it’s common to set <span class="math inline">\(\alpha\)</span> as .05, the standard value for <span class="math inline">\(\beta\)</span> is .2 - that is, we are willing to accept that 20% of the time we will fail to detect a true effect. We will return to this below when we discuss statistical power in Section <a href="ci-effect-size-power.html#statistical-power">10.3</a>, which is the complement of Type II error.</p>
</div>
</div>
<div id="what-does-a-significant-result-mean" class="section level3">
<h3><span class="header-section-number">9.3.7</span> What does a significant result mean?</h3>
<p>There is a great deal of confusion about what p-values actually mean (Gigerenzer, 2004). Let’s say that we do an experiment comparing the means between conditions, and we find a difference with a p-value of .01. There are a number of possible interpretations.</p>
<div id="does-it-mean-that-the-probability-of-the-null-hypothesis-being-true-is-.01" class="section level4">
<h4><span class="header-section-number">9.3.7.1</span> Does it mean that the probability of the null hypothesis being true is .01?</h4>
<p>No. Remember that in null hypothesis testing, the p-value is the probability of the data given the null hypothesis (<span class="math inline">\(P(data|H_0)\)</span>). It does not warrant conclusions about the probability of the null hypothesis given the data (<span class="math inline">\(P(H_0|data)\)</span>). We will return to this question when we discuss Bayesian inference in a later chapter, as Bayes theorem lets us invert the conditional probability in a way that allows us to determine the latter probability.</p>
</div>
<div id="does-it-mean-that-the-probability-that-you-are-making-the-wrong-decision-is-.01" class="section level4">
<h4><span class="header-section-number">9.3.7.2</span> Does it mean that the probability that you are making the wrong decision is .01?</h4>
<p>No. This would be <span class="math inline">\(P(H_0|data)\)</span>, but remember as above that p-values are probabilities of data under <span class="math inline">\(H_0\)</span>, not probabilities of hypotheses.</p>
</div>
<div id="does-it-mean-that-if-you-ran-the-study-again-you-would-obtain-the-same-result-99-of-the-time" class="section level4">
<h4><span class="header-section-number">9.3.7.3</span> Does it mean that if you ran the study again, you would obtain the same result 99% of the time?</h4>
<p>No. The p-value is a statement about the likelihood of a particular dataset under the null; it does not allow us to make inferences about the likelihood of future events such as replication.</p>
</div>
<div id="does-it-mean-that-you-have-found-a-meaningful-effect" class="section level4">
<h4><span class="header-section-number">9.3.7.4</span> Does it mean that you have found a meaningful effect?</h4>
<p>No. There is an important distinction between <em>statistical significance</em> and <em>practical significance</em>. As an example, let’s say that we performed a randomized controlled trial to examine the effect of a particular diet on body weight, and we find a statistically significant effect at p&lt;.05. What this doesn’t tell us is how much weight was actually lost, which we refer to as the <em>effect size</em> (to be discussed in more detail in Chapter <a href="ci-effect-size-power.html#ci-effect-size-power">10</a>). If we think about a study of weight loss, then we probably don’t think that the loss of ten ounces (i.e. the weight of a bag of potato chips) is practically significant. Let’s look at our ability to detect a significant difference of 1 ounce as the sample size increases.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create simulated data for weight loss trial</span>

weightLossTrial &lt;-<span class="st"> </span><span class="cf">function</span>(nPerGroup, <span class="dt">weightLossOz =</span> <span class="dv">1</span>) {
  <span class="co"># mean and SD in Kg based on NHANES adult dataset</span>
  kgToOz &lt;-<span class="st"> </span><span class="fl">35.27396195</span> <span class="co"># conversion constant for Kg to Oz</span>
  meanOz &lt;-<span class="st"> </span><span class="fl">81.78</span> <span class="op">*</span><span class="st"> </span>kgToOz
  sdOz &lt;-<span class="st"> </span><span class="fl">21.29</span> <span class="op">*</span><span class="st"> </span>kgToOz
  <span class="co"># create data</span>
  controlGroup &lt;-<span class="st"> </span><span class="kw">rnorm</span>(nPerGroup) <span class="op">*</span><span class="st"> </span>sdOz <span class="op">+</span><span class="st"> </span>meanOz
  expGroup &lt;-<span class="st"> </span><span class="kw">rnorm</span>(nPerGroup) <span class="op">*</span><span class="st"> </span>sdOz <span class="op">+</span><span class="st"> </span>meanOz <span class="op">-</span><span class="st"> </span>weightLossOz
  ttResult &lt;-<span class="st"> </span><span class="kw">t.test</span>(expGroup, controlGroup)
  <span class="kw">return</span>(<span class="kw">c</span>(
    nPerGroup, weightLossOz, ttResult<span class="op">$</span>p.value,
    <span class="kw">diff</span>(ttResult<span class="op">$</span>estimate)
  ))
}

nRuns &lt;-<span class="st"> </span><span class="dv">1000</span>
sampSizes &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">**</span><span class="kw">seq</span>(<span class="dv">5</span>,<span class="dv">17</span>) <span class="co"># powers of 2</span>

simResults &lt;-<span class="st"> </span><span class="kw">c</span>() ## create an empty list to add results onto
<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(sampSizes)) {
  tmpResults &lt;-<span class="st"> </span><span class="kw">replicate</span>(
    nRuns,
    <span class="kw">weightLossTrial</span>(sampSizes[i], <span class="dt">weightLossOz =</span> <span class="dv">10</span>)
  )
  summaryResults &lt;-<span class="st"> </span><span class="kw">c</span>(
    tmpResults[<span class="dv">1</span>, <span class="dv">1</span>], tmpResults[<span class="dv">2</span>, <span class="dv">1</span>],
    <span class="kw">sum</span>(tmpResults[<span class="dv">3</span>, ] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>),
    <span class="kw">mean</span>(tmpResults[<span class="dv">4</span>, ])
  )
  simResults &lt;-<span class="st"> </span><span class="kw">rbind</span>(simResults, summaryResults)
}


simResultsDf &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">as.tibble</span>(simResults) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">rename</span>(
    <span class="dt">sampleSize =</span> V1, 
    <span class="dt">effectSizeLbs =</span> V2,
    <span class="dt">nSigResults =</span> V3, 
    <span class="dt">meanEffect =</span> V4
  ) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pSigResult =</span> nSigResults <span class="op">/</span><span class="st"> </span>nRuns)</code></pre></div>
<p>Figure <a href="hypothesis-testing.html#fig:sigResults">9.8</a> shows how the proportion of significant results increases as the sample size increases, such that with a very large sample size (about 262,000 total subjects), we will find a significant result in more than 90% of studies when there is a 1 ounce weight loss. While these are statistically significant, most physicians would not consider a weight loss of one ounce to be practically or clinically significant. We will explore this relationship in more detail when we return to the concept of <em>statistical power</em> in Section <a href="ci-effect-size-power.html#statistical-power">10.3</a>, but it should already be clear from this example that statistical significance is not necessarily indicative of practical significance.</p>
<div class="figure"><span id="fig:sigResults"></span>
<img src="StatsThinking21_files/figure-html/sigResults-1.png" alt="The proportion of signifcant results for a very small change (1 ounce, which is about .001 standard deviations) as a function of sample size." width="768" height="50%" />
<p class="caption">
Figure 9.8: The proportion of signifcant results for a very small change (1 ounce, which is about .001 standard deviations) as a function of sample size.
</p>
</div>
</div>
</div>
</div>
<div id="nhst-in-a-modern-context-multiple-testing" class="section level2">
<h2><span class="header-section-number">9.4</span> NHST in a modern context: Multiple testing</h2>
<p>So far we have discussed examples where we are interested in testing a single statistical hypothesis, and this is consistent with traditional science which often measured only a few variables at a time. However, in modern science we can often measure millions of variables per individual. For example, in genetic studies that quantify the entire genome, there may be many millions of measures per individual, and in brain imaging we often collect data from more than 100,000 locations in the brain at once. When standard hypothesis testing is applied in these contexts, bad things can happen unless we take appropriate care.</p>
<p>Let’s look at an example to see how this might work. There is great interest in understanding the genetic factors that can predispose individuals to major mental illnesses such as schizophrenia, because we know that about 80% of the variation between individuals in the presence of schizophrenia is due to genetic differences. The Human Genome Project and the ensuing revolution in genome science has provided tools to examine the many ways in which humans differ from one another in their genomes. One approach that has been used in recent years is known as a <em>genome-wide association study</em> (GWAS), in which the genome of each individual is characterized at one million or more places in their genome to determine which letters of the genetic code (which we call “variants”) they have at that location. After these have been determined, the researchers perform a statistical test at each location in the genome to determine whether people diagnosed with schizoprenia are more or less likely to have one specific variant at that location.</p>
<p>Let’s imagine what would happen if the researchers simply asked whether the test was significant at p&lt;.05 at each location, when in fact there is no true effect at any of the locations. To do this, we generate a large number of simulated t values from a null distribution, and ask how many of them are significant at p&lt;.05. Let’s do this many times, and each time count up how many of the tests come out as significant (see Figure <a href="hypothesis-testing.html#fig:nullSim">9.9</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># simulate 1500 studies with 10,000 tests each, thresholded at p &lt; .05</span>

nRuns &lt;-<span class="st"> </span><span class="dv">1500</span> <span class="co"># number of simulated studies to run</span>
nTests &lt;-<span class="st"> </span><span class="dv">10000</span> <span class="co"># number of simulated genes to test in each run</span>

uncAlpha &lt;-<span class="st"> </span><span class="fl">0.05</span> <span class="co"># alpha level</span>

uncOutcome &lt;-<span class="st"> </span><span class="kw">replicate</span>(nRuns, <span class="kw">sum</span>(<span class="kw">rnorm</span>(nTests) <span class="op">&lt;</span><span class="st"> </span><span class="kw">qnorm</span>(uncAlpha)))

<span class="kw">sprintf</span>(<span class="st">&quot;mean proportion of significant tests per run: %0.2f&quot;</span>, <span class="kw">mean</span>(uncOutcome) <span class="op">/</span><span class="st"> </span>nTests)</code></pre></div>
<pre><code>## [1] &quot;mean proportion of significant tests per run: 0.05&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute proportion of studies with at least one false positive result,</span>
<span class="co"># known as the familywise error rate</span>
<span class="kw">sprintf</span>(<span class="st">&quot;familywise error rate: %0.3f&quot;</span>, <span class="kw">mean</span>(uncOutcome <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>))</code></pre></div>
<pre><code>## [1] &quot;familywise error rate: 1.000&quot;</code></pre>
<div class="figure"><span id="fig:nullSim"></span>
<img src="StatsThinking21_files/figure-html/nullSim-1.png" alt="A histogram of the number of significant results in each set of 1 million statistical tests, when there is in fact no true effect." width="384" height="50%" />
<p class="caption">
Figure 9.9: A histogram of the number of significant results in each set of 1 million statistical tests, when there is in fact no true effect.
</p>
</div>
<p>This shows that about 5% of all of the tests were significant in each run, meaning that if we were to use p &lt; .05 as our threshold for statistical significance, then even if there were no truly significant relationships present, we would still “find” about 500 genes that were seemingly significant (the expected number of significant results is simply <span class="math inline">\(n * \alpha\)</span>). That is because while we controlled for the error per test, we didn’t control the <em>familywise error</em>, or the error across all of the tests, which is what we really want to control if we are going to be looking at the results from a large number of tests. Using p&lt;.05, our familywise error rate in the above example is one – that is, we are pretty much guaranteed to make at least one error in any particular study.</p>
<p>A simple way to control for the familywise error is to divide the alpha level by the number of tests; this is known as the <em>Bonferroni</em> correction, named after the Italian statistician Carlo Bonferroni. Using the data from our example above, we see in Figure <a href="hypothesis-testing.html#fig:bonferroniSim">9.10</a> that only about 5 percent of studies show any significant results using the corrected alpha level of 0.000005 instead of the nominal level of .05. We have effectively controlled the familywise error, such that the probability of making <em>any</em> errors in our study is controlled at right around .05.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute Bonferroni-corrected alpha</span>
corAlpha &lt;-<span class="st"> </span><span class="fl">0.05</span> <span class="op">/</span><span class="st"> </span>nTests

corOutcome &lt;-<span class="st"> </span><span class="kw">replicate</span>(nRuns, <span class="kw">sum</span>(<span class="kw">rnorm</span>(nTests) <span class="op">&lt;</span><span class="st"> </span>(<span class="kw">qnorm</span>(corAlpha))))

<span class="kw">sprintf</span>(<span class="st">&quot;corrected familywise error rate: %0.3f&quot;</span>, <span class="kw">mean</span>(corOutcome <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>))</code></pre></div>
<pre><code>## [1] &quot;corrected familywise error rate: 0.046&quot;</code></pre>
<div class="figure"><span id="fig:bonferroniSim"></span>
<img src="StatsThinking21_files/figure-html/bonferroniSim-1.png" alt="A histogram of the number of significant results across all simulation runs after applying the Bonferroni correction for multiple tests." width="384" height="50%" />
<p class="caption">
Figure 9.10: A histogram of the number of significant results across all simulation runs after applying the Bonferroni correction for multiple tests.
</p>
</div>
</div>
<div id="suggested-readings-6" class="section level2">
<h2><span class="header-section-number">9.5</span> Suggested readings</h2>
<ul>
<li><a href="https://library.mpib-berlin.mpg.de/ft/gg/GG_Mindless_2004.pdf">Mindless Statistics, by Gerd Gigerenzer</a></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="resampling-and-simulation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ci-effect-size-power.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/poldrack/psych10-book/edit/master/09-HypothesisTesting.Rmd",
"text": "Edit"
},
"download": ["StatsThinking21.pdf", "StatsThinking21.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
