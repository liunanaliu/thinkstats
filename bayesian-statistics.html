<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Statistical Thinking for the 21st Century</title>
  <meta name="description" content="Statistical Thinking for the 21st Century">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Statistical Thinking for the 21st Century" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="poldrack/psych10-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistical Thinking for the 21st Century" />
  
  
  

<meta name="author" content="Copyright 2018 Russell A. Poldrack">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ci-effect-size-power.html">
<link rel="next" href="modeling-categorical-relationships.html">
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="book_assets/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="book_assets/viz-0.3/viz.js"></script>
<link href="book_assets/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="book_assets/grViz-binding-1.0.0/grViz.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129414074-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129414074-1');
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#why-does-this-book-exist"><i class="fa fa-check"></i><b>0.1</b> Why does this book exist?</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#youre-not-a-statistician---why-should-we-listen-to-you"><i class="fa fa-check"></i><b>0.2</b> You’re not a statistician - why should we listen to you?</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#why-r"><i class="fa fa-check"></i><b>0.3</b> Why R?</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#the-golden-age-of-data"><i class="fa fa-check"></i><b>0.4</b> The golden age of data</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#an-open-source-book"><i class="fa fa-check"></i><b>0.5</b> An open source book</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.6</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-statistical-thinking"><i class="fa fa-check"></i><b>1.1</b> What is statistical thinking?</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#what-can-statistics-do-for-us"><i class="fa fa-check"></i><b>1.2</b> What can statistics do for us?</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#fundamental-concepts-of-statistics"><i class="fa fa-check"></i><b>1.3</b> Fundamental concepts of statistics</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#learning-from-data"><i class="fa fa-check"></i><b>1.3.1</b> Learning from data</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#aggregation"><i class="fa fa-check"></i><b>1.3.2</b> Aggregation</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#uncertainty"><i class="fa fa-check"></i><b>1.3.3</b> Uncertainty</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#sampling"><i class="fa fa-check"></i><b>1.3.4</b> Sampling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#causality-and-statistics"><i class="fa fa-check"></i><b>1.4</b> Causality and statistics</a></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#suggested-readings"><i class="fa fa-check"></i><b>1.5</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="working-with-data.html"><a href="working-with-data.html"><i class="fa fa-check"></i><b>2</b> Working with data</a><ul>
<li class="chapter" data-level="2.1" data-path="working-with-data.html"><a href="working-with-data.html#what-are-data"><i class="fa fa-check"></i><b>2.1</b> What are data?</a><ul>
<li class="chapter" data-level="2.1.1" data-path="working-with-data.html"><a href="working-with-data.html#qualitative-data"><i class="fa fa-check"></i><b>2.1.1</b> Qualitative data</a></li>
<li class="chapter" data-level="2.1.2" data-path="working-with-data.html"><a href="working-with-data.html#quantitative-data"><i class="fa fa-check"></i><b>2.1.2</b> Quantitative data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="working-with-data.html"><a href="working-with-data.html#scales-of-measurement"><i class="fa fa-check"></i><b>2.2</b> Scales of measurement</a><ul>
<li class="chapter" data-level="2.2.1" data-path="working-with-data.html"><a href="working-with-data.html#why-do-scales-of-measurement-matter"><i class="fa fa-check"></i><b>2.2.1</b> Why do scales of measurement matter?</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="working-with-data.html"><a href="working-with-data.html#what-makes-a-good-measurement"><i class="fa fa-check"></i><b>2.3</b> What makes a good measurement?</a><ul>
<li class="chapter" data-level="2.3.1" data-path="working-with-data.html"><a href="working-with-data.html#reliability"><i class="fa fa-check"></i><b>2.3.1</b> <em>Reliability</em></a></li>
<li class="chapter" data-level="2.3.2" data-path="working-with-data.html"><a href="working-with-data.html#validity"><i class="fa fa-check"></i><b>2.3.2</b> <em>Validity</em></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="working-with-data.html"><a href="working-with-data.html#suggested-readings-1"><i class="fa fa-check"></i><b>2.4</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a><ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#what-is-probability"><i class="fa fa-check"></i><b>3.1</b> What is probability?</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#how-do-we-determine-probabilities"><i class="fa fa-check"></i><b>3.2</b> How do we determine probabilities?</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability.html"><a href="probability.html#personal-opinion"><i class="fa fa-check"></i><b>3.2.1</b> Personal opinion</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability.html"><a href="probability.html#empirical-frequency"><i class="fa fa-check"></i><b>3.2.2</b> Empirical frequency</a></li>
<li class="chapter" data-level="3.2.3" data-path="probability.html"><a href="probability.html#classical-probability"><i class="fa fa-check"></i><b>3.2.3</b> Classical probability</a></li>
<li class="chapter" data-level="3.2.4" data-path="probability.html"><a href="probability.html#solving-de-meres-problem"><i class="fa fa-check"></i><b>3.2.4</b> Solving de Méré’s problem</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#probability-distributions"><i class="fa fa-check"></i><b>3.3</b> Probability distributions</a><ul>
<li class="chapter" data-level="3.3.1" data-path="probability.html"><a href="probability.html#cumulative-probability-distributions"><i class="fa fa-check"></i><b>3.3.1</b> Cumulative probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.4</b> Conditional probability</a></li>
<li class="chapter" data-level="3.5" data-path="probability.html"><a href="probability.html#computing-conditional-probabilities-from-data"><i class="fa fa-check"></i><b>3.5</b> Computing conditional probabilities from data</a></li>
<li class="chapter" data-level="3.6" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>3.6</b> Independence</a></li>
<li class="chapter" data-level="3.7" data-path="probability.html"><a href="probability.html#bayestheorem"><i class="fa fa-check"></i><b>3.7</b> Reversing a conditional probability: Bayes’ rule</a></li>
<li class="chapter" data-level="3.8" data-path="probability.html"><a href="probability.html#learning-from-data-1"><i class="fa fa-check"></i><b>3.8</b> Learning from data</a></li>
<li class="chapter" data-level="3.9" data-path="probability.html"><a href="probability.html#odds-and-odds-ratios"><i class="fa fa-check"></i><b>3.9</b> Odds and odds ratios</a></li>
<li class="chapter" data-level="3.10" data-path="probability.html"><a href="probability.html#what-do-probabilities-mean"><i class="fa fa-check"></i><b>3.10</b> What do probabilities mean?</a></li>
<li class="chapter" data-level="3.11" data-path="probability.html"><a href="probability.html#suggested-readings-2"><i class="fa fa-check"></i><b>3.11</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="summarizing-data.html"><a href="summarizing-data.html"><i class="fa fa-check"></i><b>4</b> Summarizing data</a><ul>
<li class="chapter" data-level="4.1" data-path="summarizing-data.html"><a href="summarizing-data.html#why-summarize-data"><i class="fa fa-check"></i><b>4.1</b> Why summarize data?</a></li>
<li class="chapter" data-level="4.2" data-path="summarizing-data.html"><a href="summarizing-data.html#summarizing-data-using-tables"><i class="fa fa-check"></i><b>4.2</b> Summarizing data using tables</a><ul>
<li class="chapter" data-level="4.2.1" data-path="summarizing-data.html"><a href="summarizing-data.html#frequency-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Frequency distributions</a></li>
<li class="chapter" data-level="4.2.2" data-path="summarizing-data.html"><a href="summarizing-data.html#cumulative-distributions"><i class="fa fa-check"></i><b>4.2.2</b> Cumulative distributions</a></li>
<li class="chapter" data-level="4.2.3" data-path="summarizing-data.html"><a href="summarizing-data.html#plotting-histograms"><i class="fa fa-check"></i><b>4.2.3</b> Plotting histograms</a></li>
<li class="chapter" data-level="4.2.4" data-path="summarizing-data.html"><a href="summarizing-data.html#histogram-bins"><i class="fa fa-check"></i><b>4.2.4</b> Histogram bins</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="summarizing-data.html"><a href="summarizing-data.html#idealized-representations-of-distributions"><i class="fa fa-check"></i><b>4.3</b> Idealized representations of distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="summarizing-data.html"><a href="summarizing-data.html#skewness"><i class="fa fa-check"></i><b>4.3.1</b> Skewness</a></li>
<li class="chapter" data-level="4.3.2" data-path="summarizing-data.html"><a href="summarizing-data.html#long-tailed-distributions"><i class="fa fa-check"></i><b>4.3.2</b> Long-tailed distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summarizing-data.html"><a href="summarizing-data.html#suggested-readings-3"><i class="fa fa-check"></i><b>4.4</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html"><i class="fa fa-check"></i><b>5</b> Fitting models to data</a><ul>
<li class="chapter" data-level="5.1" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#what-is-a-model"><i class="fa fa-check"></i><b>5.1</b> What is a model?</a></li>
<li class="chapter" data-level="5.2" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#statistical-modeling-an-example"><i class="fa fa-check"></i><b>5.2</b> Statistical modeling: An example</a><ul>
<li class="chapter" data-level="5.2.1" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#improving-our-model"><i class="fa fa-check"></i><b>5.2.1</b> Improving our model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#what-makes-a-model-good"><i class="fa fa-check"></i><b>5.3</b> What makes a model “good”?</a></li>
<li class="chapter" data-level="5.4" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#overfitting"><i class="fa fa-check"></i><b>5.4</b> Can a model be too good?</a></li>
<li class="chapter" data-level="5.5" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#the-simplest-model-the-mean"><i class="fa fa-check"></i><b>5.5</b> The simplest model: The mean</a><ul>
<li class="chapter" data-level="5.5.1" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#the-median"><i class="fa fa-check"></i><b>5.5.1</b> The median</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#the-mode"><i class="fa fa-check"></i><b>5.6</b> The mode</a></li>
<li class="chapter" data-level="5.7" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#variability-how-well-does-the-mean-fit-the-data"><i class="fa fa-check"></i><b>5.7</b> Variability: How well does the mean fit the data?</a></li>
<li class="chapter" data-level="5.8" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#using-simulations-to-understand-statistics"><i class="fa fa-check"></i><b>5.8</b> Using simulations to understand statistics</a></li>
<li class="chapter" data-level="5.9" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#z-scores"><i class="fa fa-check"></i><b>5.9</b> Z-scores</a><ul>
<li class="chapter" data-level="5.9.1" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#interpreting-z-scores"><i class="fa fa-check"></i><b>5.9.1</b> Interpreting Z-scores</a></li>
<li class="chapter" data-level="5.9.2" data-path="fitting-models-to-data.html"><a href="fitting-models-to-data.html#standardized-scores"><i class="fa fa-check"></i><b>5.9.2</b> Standardized scores</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>6</b> Data Visualization</a><ul>
<li class="chapter" data-level="6.1" data-path="data-visualization.html"><a href="data-visualization.html#how-data-visualization-can-save-lives"><i class="fa fa-check"></i><b>6.1</b> How data visualization can save lives</a></li>
<li class="chapter" data-level="6.2" data-path="data-visualization.html"><a href="data-visualization.html#anatomy-of-a-plot"><i class="fa fa-check"></i><b>6.2</b> Anatomy of a plot</a></li>
<li class="chapter" data-level="6.3" data-path="data-visualization.html"><a href="data-visualization.html#plotting-in-r-using-ggplot"><i class="fa fa-check"></i><b>6.3</b> Plotting in R using ggplot</a></li>
<li class="chapter" data-level="6.4" data-path="data-visualization.html"><a href="data-visualization.html#principles-of-good-visualization"><i class="fa fa-check"></i><b>6.4</b> Principles of good visualization</a><ul>
<li class="chapter" data-level="6.4.1" data-path="data-visualization.html"><a href="data-visualization.html#show-the-data-and-make-them-stand-out"><i class="fa fa-check"></i><b>6.4.1</b> Show the data and make them stand out</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="data-visualization.html"><a href="data-visualization.html#maximize-the-dataink-ratio"><i class="fa fa-check"></i><b>6.5</b> Maximize the data/ink ratio</a></li>
<li class="chapter" data-level="6.6" data-path="data-visualization.html"><a href="data-visualization.html#avoid-chartjunk"><i class="fa fa-check"></i><b>6.6</b> Avoid chartjunk</a></li>
<li class="chapter" data-level="6.7" data-path="data-visualization.html"><a href="data-visualization.html#avoid-distorting-the-data"><i class="fa fa-check"></i><b>6.7</b> Avoid distorting the data</a></li>
<li class="chapter" data-level="6.8" data-path="data-visualization.html"><a href="data-visualization.html#the-lie-factor"><i class="fa fa-check"></i><b>6.8</b> The lie factor</a></li>
<li class="chapter" data-level="6.9" data-path="data-visualization.html"><a href="data-visualization.html#remember-human-limitations"><i class="fa fa-check"></i><b>6.9</b> Remember human limitations</a><ul>
<li class="chapter" data-level="6.9.1" data-path="data-visualization.html"><a href="data-visualization.html#perceptual-limitations"><i class="fa fa-check"></i><b>6.9.1</b> Perceptual limitations</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="data-visualization.html"><a href="data-visualization.html#correcting-for-other-factors"><i class="fa fa-check"></i><b>6.10</b> Correcting for other factors</a></li>
<li class="chapter" data-level="6.11" data-path="data-visualization.html"><a href="data-visualization.html#suggested-readings-and-videos"><i class="fa fa-check"></i><b>6.11</b> Suggested readings and videos</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sampling-1.html"><a href="sampling-1.html"><i class="fa fa-check"></i><b>7</b> Sampling</a><ul>
<li class="chapter" data-level="7.1" data-path="sampling-1.html"><a href="sampling-1.html#how-do-we-sample"><i class="fa fa-check"></i><b>7.1</b> How do we sample?</a></li>
<li class="chapter" data-level="7.2" data-path="sampling-1.html"><a href="sampling-1.html#sampling-error"><i class="fa fa-check"></i><b>7.2</b> Sampling error</a></li>
<li class="chapter" data-level="7.3" data-path="sampling-1.html"><a href="sampling-1.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>7.3</b> Standard error of the mean</a></li>
<li class="chapter" data-level="7.4" data-path="sampling-1.html"><a href="sampling-1.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>7.4</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="7.5" data-path="sampling-1.html"><a href="sampling-1.html#confidence-intervals"><i class="fa fa-check"></i><b>7.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="7.6" data-path="sampling-1.html"><a href="sampling-1.html#suggested-readings-4"><i class="fa fa-check"></i><b>7.6</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html"><i class="fa fa-check"></i><b>8</b> Resampling and simulation</a><ul>
<li class="chapter" data-level="8.1" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.1</b> Monte Carlo simulation</a></li>
<li class="chapter" data-level="8.2" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#randomness-in-statistics"><i class="fa fa-check"></i><b>8.2</b> Randomness in statistics</a></li>
<li class="chapter" data-level="8.3" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#generating-random-numbers"><i class="fa fa-check"></i><b>8.3</b> Generating random numbers</a></li>
<li class="chapter" data-level="8.4" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#using-monte-carlo-simulation"><i class="fa fa-check"></i><b>8.4</b> Using Monte Carlo simulation</a></li>
<li class="chapter" data-level="8.5" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#using-simulation-for-statistics-the-bootstrap"><i class="fa fa-check"></i><b>8.5</b> Using simulation for statistics: The bootstrap</a><ul>
<li class="chapter" data-level="8.5.1" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#computing-the-bootstrap"><i class="fa fa-check"></i><b>8.5.1</b> Computing the bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="resampling-and-simulation.html"><a href="resampling-and-simulation.html#suggested-readings-5"><i class="fa fa-check"></i><b>8.6</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>9</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="9.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#null-hypothesis-statistical-testing-nhst"><i class="fa fa-check"></i><b>9.1</b> Null Hypothesis Statistical Testing (NHST)</a></li>
<li class="chapter" data-level="9.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#null-hypothesis-statistical-testing-an-example"><i class="fa fa-check"></i><b>9.2</b> Null hypothesis statistical testing: An example</a></li>
<li class="chapter" data-level="9.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#the-process-of-null-hypothesis-testing"><i class="fa fa-check"></i><b>9.3</b> The process of null hypothesis testing</a><ul>
<li class="chapter" data-level="9.3.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-1-formulate-a-hypothesis"><i class="fa fa-check"></i><b>9.3.1</b> Step 1: Formulate a hypothesis</a></li>
<li class="chapter" data-level="9.3.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-2-collect-some-data"><i class="fa fa-check"></i><b>9.3.2</b> Step 2: Collect some data</a></li>
<li class="chapter" data-level="9.3.3" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-3-specify-the-null-and-alternative-hypotheses"><i class="fa fa-check"></i><b>9.3.3</b> Step 3: Specify the null and alternative hypotheses</a></li>
<li class="chapter" data-level="9.3.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-4-fit-a-model-to-the-data-and-compute-a-test-statistic"><i class="fa fa-check"></i><b>9.3.4</b> Step 4: Fit a model to the data and compute a test statistic</a></li>
<li class="chapter" data-level="9.3.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-5-determine-the-probability-of-the-data-under-the-null-hypothesis"><i class="fa fa-check"></i><b>9.3.5</b> Step 5: Determine the probability of the data under the null hypothesis</a></li>
<li class="chapter" data-level="9.3.6" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#step-6-assess-the-statistical-significance-of-the-result"><i class="fa fa-check"></i><b>9.3.6</b> Step 6: Assess the “statistical significance” of the result</a></li>
<li class="chapter" data-level="9.3.7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#what-does-a-significant-result-mean"><i class="fa fa-check"></i><b>9.3.7</b> What does a significant result mean?</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#nhst-in-a-modern-context-multiple-testing"><i class="fa fa-check"></i><b>9.4</b> NHST in a modern context: Multiple testing</a></li>
<li class="chapter" data-level="9.5" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#suggested-readings-6"><i class="fa fa-check"></i><b>9.5</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html"><i class="fa fa-check"></i><b>10</b> Confidence intervals, effect sizes, and statistical power</a><ul>
<li class="chapter" data-level="10.1" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#confidence-intervals-1"><i class="fa fa-check"></i><b>10.1</b> Confidence intervals</a><ul>
<li class="chapter" data-level="10.1.1" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#confidence-intervals-using-the-normal-distribution"><i class="fa fa-check"></i><b>10.1.1</b> Confidence intervals using the normal distribution</a></li>
<li class="chapter" data-level="10.1.2" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#confidence-intervals-using-the-t-distribution"><i class="fa fa-check"></i><b>10.1.2</b> Confidence intervals using the t distribution</a></li>
<li class="chapter" data-level="10.1.3" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#confidence-intervals-and-sample-size"><i class="fa fa-check"></i><b>10.1.3</b> Confidence intervals and sample size</a></li>
<li class="chapter" data-level="10.1.4" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#computing-confidence-intervals-using-the-bootstrap"><i class="fa fa-check"></i><b>10.1.4</b> Computing confidence intervals using the bootstrap</a></li>
<li class="chapter" data-level="10.1.5" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#relation-of-confidence-intervals-to-hypothesis-tests"><i class="fa fa-check"></i><b>10.1.5</b> Relation of confidence intervals to hypothesis tests</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#effect-sizes"><i class="fa fa-check"></i><b>10.2</b> Effect sizes</a><ul>
<li class="chapter" data-level="10.2.1" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#cohens-d"><i class="fa fa-check"></i><b>10.2.1</b> Cohen’s D</a></li>
<li class="chapter" data-level="10.2.2" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#pearsons-r"><i class="fa fa-check"></i><b>10.2.2</b> Pearson’s r</a></li>
<li class="chapter" data-level="10.2.3" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#odds-ratio"><i class="fa fa-check"></i><b>10.2.3</b> Odds ratio</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#statistical-power"><i class="fa fa-check"></i><b>10.3</b> Statistical power</a><ul>
<li class="chapter" data-level="10.3.1" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#power-analysis"><i class="fa fa-check"></i><b>10.3.1</b> Power analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ci-effect-size-power.html"><a href="ci-effect-size-power.html#suggested-readings-7"><i class="fa fa-check"></i><b>10.4</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html"><i class="fa fa-check"></i><b>11</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="11.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#generative-models"><i class="fa fa-check"></i><b>11.1</b> Generative models</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayes-theorem-and-inverse-inference"><i class="fa fa-check"></i><b>11.2</b> Bayes’ theorem and inverse inference</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#doing-bayesian-estimation"><i class="fa fa-check"></i><b>11.3</b> Doing Bayesian estimation</a><ul>
<li class="chapter" data-level="11.3.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#specifying-the-prior"><i class="fa fa-check"></i><b>11.3.1</b> Specifying the prior</a></li>
<li class="chapter" data-level="11.3.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#collect-some-data"><i class="fa fa-check"></i><b>11.3.2</b> Collect some data</a></li>
<li class="chapter" data-level="11.3.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-likelihood"><i class="fa fa-check"></i><b>11.3.3</b> Computing the likelihood</a></li>
<li class="chapter" data-level="11.3.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-marginal-likelihood"><i class="fa fa-check"></i><b>11.3.4</b> Computing the marginal likelihood</a></li>
<li class="chapter" data-level="11.3.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-posterior"><i class="fa fa-check"></i><b>11.3.5</b> Computing the posterior</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#estimating-posterior-distributions"><i class="fa fa-check"></i><b>11.4</b> Estimating posterior distributions</a><ul>
<li class="chapter" data-level="11.4.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#specifying-the-prior-1"><i class="fa fa-check"></i><b>11.4.1</b> Specifying the prior</a></li>
<li class="chapter" data-level="11.4.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#collect-some-data-1"><i class="fa fa-check"></i><b>11.4.2</b> Collect some data</a></li>
<li class="chapter" data-level="11.4.3" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-likelihood-1"><i class="fa fa-check"></i><b>11.4.3</b> Computing the likelihood</a></li>
<li class="chapter" data-level="11.4.4" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-marginal-likelihood-1"><i class="fa fa-check"></i><b>11.4.4</b> Computing the marginal likelihood</a></li>
<li class="chapter" data-level="11.4.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#computing-the-posterior-1"><i class="fa fa-check"></i><b>11.4.5</b> Computing the posterior</a></li>
<li class="chapter" data-level="11.4.6" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#maximum-a-posteriori-map-estimation"><i class="fa fa-check"></i><b>11.4.6</b> Maximum a posteriori (MAP) estimation</a></li>
<li class="chapter" data-level="11.4.7" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#credible-intervals"><i class="fa fa-check"></i><b>11.4.7</b> Credible intervals</a></li>
<li class="chapter" data-level="11.4.8" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#effects-of-different-priors"><i class="fa fa-check"></i><b>11.4.8</b> Effects of different priors</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#choosing-a-prior"><i class="fa fa-check"></i><b>11.5</b> Choosing a prior</a></li>
<li class="chapter" data-level="11.6" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayesian-hypothesis-testing"><i class="fa fa-check"></i><b>11.6</b> Bayesian hypothesis testing</a><ul>
<li class="chapter" data-level="11.6.1" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayes-factors"><i class="fa fa-check"></i><b>11.6.1</b> Bayes factors</a></li>
<li class="chapter" data-level="11.6.2" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#bayes-factors-for-statistical-hypotheses"><i class="fa fa-check"></i><b>11.6.2</b> Bayes factors for statistical hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="bayesian-statistics.html"><a href="bayesian-statistics.html#suggested-readings-8"><i class="fa fa-check"></i><b>11.7</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html"><i class="fa fa-check"></i><b>12</b> Modeling categorical relationships</a><ul>
<li class="chapter" data-level="12.1" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#example-candy-colors"><i class="fa fa-check"></i><b>12.1</b> Example: Candy colors</a></li>
<li class="chapter" data-level="12.2" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#pearsons-chi-squared-test"><i class="fa fa-check"></i><b>12.2</b> Pearson’s chi-squared test</a></li>
<li class="chapter" data-level="12.3" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#contingency-tables-and-the-two-way-test"><i class="fa fa-check"></i><b>12.3</b> Contingency tables and the two-way test</a></li>
<li class="chapter" data-level="12.4" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#standardized-residuals"><i class="fa fa-check"></i><b>12.4</b> Standardized residuals</a></li>
<li class="chapter" data-level="12.5" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#odds-ratios"><i class="fa fa-check"></i><b>12.5</b> Odds ratios</a></li>
<li class="chapter" data-level="12.6" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#bayes-factor"><i class="fa fa-check"></i><b>12.6</b> Bayes factor</a></li>
<li class="chapter" data-level="12.7" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#categorical-analysis-beyond-the-2-x-2-table"><i class="fa fa-check"></i><b>12.7</b> Categorical analysis beyond the 2 X 2 table</a></li>
<li class="chapter" data-level="12.8" data-path="modeling-categorical-relationships.html"><a href="modeling-categorical-relationships.html#beware-of-simpsons-paradox"><i class="fa fa-check"></i><b>12.8</b> Beware of Simpson’s paradox</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html"><i class="fa fa-check"></i><b>13</b> Modeling continuous relationships</a><ul>
<li class="chapter" data-level="13.1" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#an-example-hate-crimes-and-income-inequality"><i class="fa fa-check"></i><b>13.1</b> An example: Hate crimes and income inequality</a><ul>
<li class="chapter" data-level="13.1.1" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#quantifying-inequality-the-gini-index"><i class="fa fa-check"></i><b>13.1.1</b> Quantifying inequality: The Gini index</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#is-income-inequality-related-to-hate-crimes"><i class="fa fa-check"></i><b>13.2</b> Is income inequality related to hate crimes?</a></li>
<li class="chapter" data-level="13.3" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#covariance-and-correlation"><i class="fa fa-check"></i><b>13.3</b> Covariance and correlation</a><ul>
<li class="chapter" data-level="13.3.1" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#hypothesis-testing-for-correlations"><i class="fa fa-check"></i><b>13.3.1</b> Hypothesis testing for correlations</a></li>
<li class="chapter" data-level="13.3.2" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#robust-correlations"><i class="fa fa-check"></i><b>13.3.2</b> Robust correlations</a></li>
<li class="chapter" data-level="13.3.3" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#bayesian-correlation-analysis"><i class="fa fa-check"></i><b>13.3.3</b> Bayesian correlation analysis</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#correlation-and-causation"><i class="fa fa-check"></i><b>13.4</b> Correlation and causation</a><ul>
<li class="chapter" data-level="13.4.1" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#causal-graphs"><i class="fa fa-check"></i><b>13.4.1</b> Causal graphs</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="modeling-continuous-relationships.html"><a href="modeling-continuous-relationships.html#suggested-readings-9"><i class="fa fa-check"></i><b>13.5</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html"><i class="fa fa-check"></i><b>14</b> The General Linear Model</a><ul>
<li class="chapter" data-level="14.1" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#linear-regression"><i class="fa fa-check"></i><b>14.1</b> Linear regression</a><ul>
<li class="chapter" data-level="14.1.1" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#regression-to-the-mean"><i class="fa fa-check"></i><b>14.1.1</b> Regression to the mean</a></li>
<li class="chapter" data-level="14.1.2" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#estimating-linear-regression-parameters"><i class="fa fa-check"></i><b>14.1.2</b> Estimating linear regression parameters</a></li>
<li class="chapter" data-level="14.1.3" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#the-relation-between-correlation-and-regression"><i class="fa fa-check"></i><b>14.1.3</b> The relation between correlation and regression</a></li>
<li class="chapter" data-level="14.1.4" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#standard-errors-for-regression-models"><i class="fa fa-check"></i><b>14.1.4</b> Standard errors for regression models</a></li>
<li class="chapter" data-level="14.1.5" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#statistical-tests-for-regression-parameters"><i class="fa fa-check"></i><b>14.1.5</b> Statistical tests for regression parameters</a></li>
<li class="chapter" data-level="14.1.6" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#quantifying-goodness-of-fit-of-the-model"><i class="fa fa-check"></i><b>14.1.6</b> Quantifying goodness of fit of the model</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#fitting-more-complex-models"><i class="fa fa-check"></i><b>14.2</b> Fitting more complex models</a></li>
<li class="chapter" data-level="14.3" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#interactions-between-variables"><i class="fa fa-check"></i><b>14.3</b> Interactions between variables</a></li>
<li class="chapter" data-level="14.4" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#what-does-predict-really-mean"><i class="fa fa-check"></i><b>14.4</b> What does “predict” really mean?</a><ul>
<li class="chapter" data-level="14.4.1" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#cross-validation"><i class="fa fa-check"></i><b>14.4.1</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="the-general-linear-model.html"><a href="the-general-linear-model.html#suggested-readings-10"><i class="fa fa-check"></i><b>14.5</b> Suggested readings</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="comparing-means.html"><a href="comparing-means.html"><i class="fa fa-check"></i><b>15</b> Comparing means</a><ul>
<li class="chapter" data-level="15.1" data-path="comparing-means.html"><a href="comparing-means.html#students-t-test"><i class="fa fa-check"></i><b>15.1</b> Student’s T test</a></li>
<li class="chapter" data-level="15.2" data-path="comparing-means.html"><a href="comparing-means.html#the-t-test-as-a-linear-model"><i class="fa fa-check"></i><b>15.2</b> The t-test as a linear model</a><ul>
<li class="chapter" data-level="15.2.1" data-path="comparing-means.html"><a href="comparing-means.html#effect-sizes-for-comparing-two-means"><i class="fa fa-check"></i><b>15.2.1</b> Effect sizes for comparing two means</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="comparing-means.html"><a href="comparing-means.html#bayes-factor-for-mean-differences"><i class="fa fa-check"></i><b>15.3</b> Bayes factor for mean differences</a></li>
<li class="chapter" data-level="15.4" data-path="comparing-means.html"><a href="comparing-means.html#paired-t-tests"><i class="fa fa-check"></i><b>15.4</b> Paired t-tests</a><ul>
<li class="chapter" data-level="15.4.1" data-path="comparing-means.html"><a href="comparing-means.html#sign-test"><i class="fa fa-check"></i><b>15.4.1</b> Sign test</a></li>
<li class="chapter" data-level="15.4.2" data-path="comparing-means.html"><a href="comparing-means.html#paired-t-test"><i class="fa fa-check"></i><b>15.4.2</b> Paired t-test</a></li>
<li class="chapter" data-level="15.4.3" data-path="comparing-means.html"><a href="comparing-means.html#the-paired-t-test-as-a-linear-model"><i class="fa fa-check"></i><b>15.4.3</b> The paired t-test as a linear model</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="comparing-means.html"><a href="comparing-means.html#comparing-more-than-two-means"><i class="fa fa-check"></i><b>15.5</b> Comparing more than two means</a><ul>
<li class="chapter" data-level="15.5.1" data-path="comparing-means.html"><a href="comparing-means.html#analysis-of-variance"><i class="fa fa-check"></i><b>15.5.1</b> Analysis of variance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="practical-example.html"><a href="practical-example.html"><i class="fa fa-check"></i><b>16</b> The process of statistical modeling: A practical example</a><ul>
<li class="chapter" data-level="16.1" data-path="practical-example.html"><a href="practical-example.html#the-process-of-statistical-modeling"><i class="fa fa-check"></i><b>16.1</b> The process of statistical modeling</a><ul>
<li class="chapter" data-level="16.1.1" data-path="practical-example.html"><a href="practical-example.html#specify-your-question-of-interest"><i class="fa fa-check"></i><b>16.1.1</b> 1: Specify your question of interest</a></li>
<li class="chapter" data-level="16.1.2" data-path="practical-example.html"><a href="practical-example.html#identify-or-collect-the-appropriate-data"><i class="fa fa-check"></i><b>16.1.2</b> 2: Identify or collect the appropriate data</a></li>
<li class="chapter" data-level="16.1.3" data-path="practical-example.html"><a href="practical-example.html#prepare-the-data-for-analysis"><i class="fa fa-check"></i><b>16.1.3</b> 3: Prepare the data for analysis</a></li>
<li class="chapter" data-level="16.1.4" data-path="practical-example.html"><a href="practical-example.html#determine-the-appropriate-model"><i class="fa fa-check"></i><b>16.1.4</b> 4. Determine the appropriate model</a></li>
<li class="chapter" data-level="16.1.5" data-path="practical-example.html"><a href="practical-example.html#fit-the-model-to-the-data"><i class="fa fa-check"></i><b>16.1.5</b> 5. Fit the model to the data</a></li>
<li class="chapter" data-level="16.1.6" data-path="practical-example.html"><a href="practical-example.html#criticize-the-model-to-make-sure-it-fits-properly"><i class="fa fa-check"></i><b>16.1.6</b> 6. Criticize the model to make sure it fits properly</a></li>
<li class="chapter" data-level="16.1.7" data-path="practical-example.html"><a href="practical-example.html#test-hypothesis-and-quantify-effect-size"><i class="fa fa-check"></i><b>16.1.7</b> 7. Test hypothesis and quantify effect size</a></li>
<li class="chapter" data-level="16.1.8" data-path="practical-example.html"><a href="practical-example.html#what-about-possible-confounds"><i class="fa fa-check"></i><b>16.1.8</b> What about possible confounds?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html"><i class="fa fa-check"></i><b>17</b> Doing reproducible research</a><ul>
<li class="chapter" data-level="17.1" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#how-we-think-science-should-work"><i class="fa fa-check"></i><b>17.1</b> How we think science should work</a></li>
<li class="chapter" data-level="17.2" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#how-science-sometimes-actually-works"><i class="fa fa-check"></i><b>17.2</b> How science (sometimes) actually works</a></li>
<li class="chapter" data-level="17.3" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#the-reproducibility-crisis-in-science"><i class="fa fa-check"></i><b>17.3</b> The reproducibility crisis in science</a><ul>
<li class="chapter" data-level="17.3.1" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#positive-predictive-value-and-statistical-significance"><i class="fa fa-check"></i><b>17.3.1</b> Positive predictive value and statistical significance</a></li>
<li class="chapter" data-level="17.3.2" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#the-winners-curse"><i class="fa fa-check"></i><b>17.3.2</b> The winner’s curse</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#questionable-research-practices"><i class="fa fa-check"></i><b>17.4</b> Questionable research practices</a><ul>
<li class="chapter" data-level="17.4.1" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#esp-or-qrp"><i class="fa fa-check"></i><b>17.4.1</b> ESP or QRP?</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#doing-reproducible-research-1"><i class="fa fa-check"></i><b>17.5</b> Doing reproducible research</a><ul>
<li class="chapter" data-level="17.5.1" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#pre-registration"><i class="fa fa-check"></i><b>17.5.1</b> Pre-registration</a></li>
<li class="chapter" data-level="17.5.2" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#reproducible-practices"><i class="fa fa-check"></i><b>17.5.2</b> Reproducible practices</a></li>
<li class="chapter" data-level="17.5.3" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#replication"><i class="fa fa-check"></i><b>17.5.3</b> Replication</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#doing-reproducible-data-analysis"><i class="fa fa-check"></i><b>17.6</b> Doing reproducible data analysis</a></li>
<li class="chapter" data-level="17.7" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#conclusion-doing-better-science"><i class="fa fa-check"></i><b>17.7</b> Conclusion: Doing better science</a></li>
<li class="chapter" data-level="17.8" data-path="doing-reproducible-research.html"><a href="doing-reproducible-research.html#suggested-readings-11"><i class="fa fa-check"></i><b>17.8</b> Suggested Readings</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Thinking for the 21st Century</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-statistics" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Bayesian statistics</h1>
<p>In this chapter we will take up the approach to statistical modeling and inference that stands in contrast to the null hypothesis testing framework that you encountered in Chapter <a href="hypothesis-testing.html#hypothesis-testing">9</a>. This is known as “Bayesian statistics” after the Reverend Thomas Bayes, whose theorem you have already encountered in Chapter <a href="probability.html#probability">3</a>. In this chapter you will learn how Bayes’ theorem provides a way of understanding data that solves many of the conceptual problems that we discussed regarding null hypothesis testing.</p>
<div id="generative-models" class="section level2">
<h2><span class="header-section-number">11.1</span> Generative models</h2>
<p>Say you are walking down the street and a friend of yours walks right by but doesn’t say hello. You would probably try to decide why this happened – Did they not see you? Are they mad at you? Are you suddenly cloaked in a magic invisibility shield? One of the basic ideas behind Bayesian statistics is that we want to infer the details of how the data are being generated, based on the data themselves. In this case, you want to use the data (i.e. the fact that your friend did not say hello) to infer the process that generated the data (e.g. whether or not they actually saw you, how they feel about you, etc).</p>
<p>The idea behind a generative model is that we observe data that are generated by a <em>latent</em> (unseen) process, usually with some amount of randomness in the process. In fact, when we take a sample of data from a population and estimate a parameter from the sample, what we are doing in essence is trying to learn the value of a latent variable (the population mean) that gives rise through sampling to the observed data (the sample mean).</p>
<p>If we know the value of the latent variable, then it’s easy to reconstruct what the observed data should look like. For example, let’s say that we are flipping a coin that we know to be fair. We can describe the coin by a binomial distribution with a value of p=0.5, and then we could generate random samples from such a distribution in order to see what the observed data should look like. However, in general we are in the opposite situation: We don’t know the value of the latent variable of interest, but we have some data that we would like to use to estimate it.</p>
</div>
<div id="bayes-theorem-and-inverse-inference" class="section level2">
<h2><span class="header-section-number">11.2</span> Bayes’ theorem and inverse inference</h2>
<p>The reason that Bayesian statistics has its name is because it takes advantage of Bayes’ theorem to make inferences from data back to some features of the (latent) model that generated the data. Let’s say that we want to know whether a coin is fair. To test this, we flip the coin 10 times and come up with 7 heads. Before this test we were pretty sure that the coin was fair (i.e., that <span class="math inline">\(p_{heads}=0.5\)</span>), but these data certainly give us pause. We already know how to compute the conditional probability that we would flip 7 or more heads out of 10 if the coin is really fair (<span class="math inline">\(P(n\ge7|p_{heads}=0.5)\)</span>), using the binomial distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute the conditional probability of 7 or more heads when p(heads)=0.5</span>
<span class="kw">sprintf</span>(
  <span class="st">&quot;p(7 or more heads | p(heads) = 0.5) = %.3f&quot;</span>,
  <span class="kw">pbinom</span>(<span class="dv">7</span>, <span class="dv">10</span>, .<span class="dv">5</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)
)</code></pre></div>
<pre><code>## [1] &quot;p(7 or more heads | p(heads) = 0.5) = 0.055&quot;</code></pre>
<p>That is an fairly small number, but this number doesn’t really answer the question that we are asking – it is telling us about the likelihood of 7 or more heads given some particular probability of heads, whereas what we really want to know is the probability of heads. This should sound familiar, as it’s exactly the situation that we were in with null hypothesis testing, which told us about the likelihood of data rather than the likelihood of hypotheses.</p>
<p>Remember that Bayes’ theorem provides us with the tool that we need to invert a conditional probability:</p>
<p><span class="math display">\[
P(H|D) = \frac{P(D|H)*P(H)}{P(D)}
\]</span></p>
<p>We can think of this theorem as having four parts:</p>
<ul>
<li>prior (<span class="math inline">\(P(H)\)</span>): Our degree of belief about hypothesis H before seeing the data D</li>
<li>likelihood (<span class="math inline">\(P(D|H)\)</span>): How likely are the observed data D under hypothesis H?</li>
<li>marginal likelihood (<span class="math inline">\(P(D)\)</span>): How likely are the observed data, combining over all possible hypotheses?</li>
<li>posterior (<span class="math inline">\(P(H|D)\)</span>): Our updated belief about hypothesis H, given the data D</li>
</ul>
<p>Here we see one of the primary differences between frequentist and Bayesian statsistics. Frequentists do not believe in the idea of a probability of a hypothesis (i.e., our degree of belief about a hypothesis) – for them, a hypothesis is either true or it isn’t. Another way to say this is that for the frequentist, the hypothesis is fixed and the data are random, which is why frequentist inference focuses on describing the probability of data given a hypothesis (i.e. the p-value). Bayesians, on the other hand, are comfortable making probability statements about both data and hypotheses.</p>
</div>
<div id="doing-bayesian-estimation" class="section level2">
<h2><span class="header-section-number">11.3</span> Doing Bayesian estimation</h2>
<p>We ultimately want to use Bayesian statistics to test hypotheses, but before we do that we need to estimate the parameters that are necessary to test the hypothesis. Here we will walk through the process of Bayesian estimation. Let’s use another screening example: Airport security screening. If you fly a lot like I do, it’s just a matter of time until one of the random explosive screenings comes back positive; I had the particularly unfortunate experience of this happening soon after September 11, 2001, when airport security staff were especially on edge.</p>
<p>What the security staff want to know is what is the likelihood that a person is carrying an explosive, given that the machine has given a positive test. Let’s walk through how to calculate this value using Bayesian analysis.</p>
<div id="specifying-the-prior" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Specifying the prior</h3>
<p>To use Bayes’ theorem, we first need to specify the prior probability for the hypothesis. In this case, we don’t know the real number but we can assume that it’s quite small. According to the <a href="https://www.faa.gov/air_traffic/by_the_numbers/media/Air_Traffic_by_the_Numbers_2018.pdf">FAA</a>, there were 971,595,898 air passengers in the U.S. in 2017. For this example, let’s say that one out of those travelers was carrying an explosive in their bag</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prior &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">971595898</span></code></pre></div>
</div>
<div id="collect-some-data" class="section level3">
<h3><span class="header-section-number">11.3.2</span> Collect some data</h3>
<p>The data are composed of the results of the explosive screening test. Let’s say that the security staff runs the bag through their testing apparatus 10 times, and it gives a positive reading on 9 of the 10 tests.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nTests &lt;-<span class="st"> </span><span class="dv">10</span>
nPositives &lt;-<span class="st"> </span><span class="dv">9</span></code></pre></div>
</div>
<div id="computing-the-likelihood" class="section level3">
<h3><span class="header-section-number">11.3.3</span> Computing the likelihood</h3>
<p>We want to compute the likelihood of the data under the hypothesis that there is an explosive in the bag. Let’s say that we know that the sensitivity of the test is 0.99 – that is, when a device is present, it will detect it 99% of the time. To determine the likelihood of our data under the hypothesis that a device is present, we can treat each test as a Bernoulli trial (that is, a trial with an outcome of true or false) with a probability of success of 0.99, which we can model using a binomial distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">likelihood &lt;-<span class="st"> </span><span class="kw">dbinom</span>(nPositives, nTests, <span class="fl">0.99</span>)
likelihood</code></pre></div>
<pre><code>## [1] 0.091</code></pre>
</div>
<div id="computing-the-marginal-likelihood" class="section level3">
<h3><span class="header-section-number">11.3.4</span> Computing the marginal likelihood</h3>
<p>We also need to know the overall likelihood of the data – that is, finding 9 positives out of 10 tests. Computing the marginal likelihood is often one of the most difficult aspects of Bayesian analysis, but for our example it’s simple because we can take advantage of the specific form of Bayes’ theorem that we introduced in Section <a href="probability.html#bayestheorem">3.7</a>:</p>
<p><span class="math display">\[
P(B|A) = \frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\neg B)*P(\neg B)}
\]</span></p>
<p>The marginal likelihood in this case is a weighted average of the likelihood of the data under either presence or absence of the explosive, multiplied by the probability of the explosive being present (i.e. the prior). In this case, let’s say that we know that the specificity of the test is 0.9, such that the likelihood of a positive result when there is no explosive is 0.1.</p>
<p>We can compute this in R as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">marginal_likelihood &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">dbinom</span>(
    <span class="dt">x =</span> nPositives, 
    <span class="dt">size =</span> nTests, 
    <span class="dt">prob =</span> <span class="fl">0.99</span>
  ) <span class="op">*</span><span class="st"> </span>prior <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">dbinom</span>(
    <span class="dt">x =</span> nPositives, 
    <span class="dt">size =</span> nTests, 
    <span class="dt">prob =</span> .<span class="dv">1</span>
  ) <span class="op">*</span><span class="st"> </span>
<span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>prior)

<span class="kw">sprintf</span>(<span class="st">&quot;marginal likelihood = %.3e&quot;</span>, marginal_likelihood)</code></pre></div>
<pre><code>## [1] &quot;marginal likelihood = 9.094e-09&quot;</code></pre>
</div>
<div id="computing-the-posterior" class="section level3">
<h3><span class="header-section-number">11.3.5</span> Computing the posterior</h3>
<p>We now have all of the parts that we need to compute the posterior probability of an explosive being present, given the observed 9 positive outcomes out of 10 tests.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">posterior &lt;-<span class="st"> </span>(likelihood <span class="op">*</span><span class="st"> </span>prior) <span class="op">/</span><span class="st"> </span>marginal_likelihood
posterior</code></pre></div>
<pre><code>## [1] 0.01</code></pre>
<p>This result shows us that the probability of an explosive in the bag is much higher than the prior, but nowhere near certainty, again highlighting the fact that testing for rare events is almost always liable to produce high numbers of false positives.</p>
</div>
</div>
<div id="estimating-posterior-distributions" class="section level2">
<h2><span class="header-section-number">11.4</span> Estimating posterior distributions</h2>
<p>In the previous example there were only two possible outcomes – the explosive is either there or it’s not – and we wanted to know which outcome was most likely given the data. However, in other cases we want to use Bayesian estimation to estimate the numeric value of a parameter. Let’s say that we want to know about the effectiveness of a new drug for pain; to test this, we can administer the drug to a group of patients and then ask them whether their pain improved or not after taking the drug. We can use Bayesian analysis to estimate the proportion of people for whom the drug will be effective using these data.</p>
<div id="specifying-the-prior-1" class="section level3">
<h3><span class="header-section-number">11.4.1</span> Specifying the prior</h3>
<p>In this case, we don’t have any prior information about the effectiveness of the drug, so we will use a <em>uniform distribution</em> as our prior, since all values are equally likely under a uniform distribution. In order to simplify the example, we will only look at a subset of 99 possible values of effectiveness (from .01 to .99, in steps of .01). Therefore, each possible value has a prior probability of 1/99.</p>
</div>
<div id="collect-some-data-1" class="section level3">
<h3><span class="header-section-number">11.4.2</span> Collect some data</h3>
<p>We need some data in order to estimate the effect of the drug. Let’s say that we administer the drug to 100 individuals, and get the following results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a table with results</span>
nResponders &lt;-<span class="st"> </span><span class="dv">64</span>
nTested &lt;-<span class="st"> </span><span class="dv">100</span>

drugDf &lt;-<span class="st"> </span><span class="kw">tibble</span>(
  <span class="dt">outcome =</span> <span class="kw">c</span>(<span class="st">&quot;improved&quot;</span>, <span class="st">&quot;not improved&quot;</span>),
  <span class="dt">number =</span> <span class="kw">c</span>(nResponders, nTested <span class="op">-</span><span class="st"> </span>nResponders)
)
<span class="kw">pander</span>(drugDf)</code></pre></div>
<table style="width:32%;">
<colgroup>
<col width="20%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">outcome</th>
<th align="center">number</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">improved</td>
<td align="center">64</td>
</tr>
<tr class="even">
<td align="center">not improved</td>
<td align="center">36</td>
</tr>
</tbody>
</table>
</div>
<div id="computing-the-likelihood-1" class="section level3">
<h3><span class="header-section-number">11.4.3</span> Computing the likelihood</h3>
<p>We can compute the likelihood of the data under any particular value of the effectiveness parameter using the <code>dbinom()</code> function in R. In Figure <a href="bayesian-statistics.html#fig:like2">11.1</a> you can see the likelihood curves over numbers of responders for several different values of <span class="math inline">\(p_{respond}\)</span>. Looking at this, it seems that our observed data are relatively more likely under the hypothesis of <span class="math inline">\(p_{respond}=0.7\)</span>, somewhat less likely under the hypothesis of <span class="math inline">\(p_{respond}=0.5\)</span>, and quite unlikely under the hypothesis of <span class="math inline">\(p_{respond}=0.3\)</span>. One of the fundamental ideas of Bayesian inference is that we will try to find the value of our parameter of interest that makes the data most likely, while also taking into account our prior knowledge.</p>
<div class="figure"><span id="fig:like2"></span>
<img src="StatsThinking21_files/figure-html/like2-1.png" alt="Likelihood of each possible number of responders under several different hypotheses (p(respond)=0.5 (red), 0.7 (green), 0.3 (black).  Observed value shown in blue." width="384" height="50%" />
<p class="caption">
Figure 11.1: Likelihood of each possible number of responders under several different hypotheses (p(respond)=0.5 (red), 0.7 (green), 0.3 (black). Observed value shown in blue.
</p>
</div>
</div>
<div id="computing-the-marginal-likelihood-1" class="section level3">
<h3><span class="header-section-number">11.4.4</span> Computing the marginal likelihood</h3>
<p>In addition to the likelihood of the data under different hypotheses, we need to know the overall likelihood of the data, combining across all hypotheses (i.e., the marginal likelihood). This marginal likelihood is primarily important beacuse it helps to ensure that the posterior values are true probabilities. In this case, our use of a set of discrete possible parameter values makes it easy to compute the marginal likelihood, because we can just compute the likelihood of each parameter value under each hypothesis and add them up.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute marginal likelihood</span>
likeDf &lt;-<span class="st"> </span>
<span class="st">  </span>likeDf <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">uniform_prior =</span> <span class="kw">array</span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">n</span>()))

<span class="co"># multiply each likelihood by prior and add them up</span>
marginal_likelihood &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">sum</span>(
    <span class="kw">dbinom</span>(
      <span class="dt">x =</span> nResponders, <span class="co"># the number who responded to the drug</span>
      <span class="dt">size =</span> <span class="dv">100</span>, <span class="co"># the number tested</span>
      likeDf<span class="op">$</span>presp <span class="co"># the likelihood of each response </span>
    ) <span class="op">*</span><span class="st"> </span>likeDf<span class="op">$</span>uniform_prior
  )

<span class="kw">sprintf</span>(<span class="st">&quot;marginal likelihood = %0.4f&quot;</span>, marginal_likelihood)</code></pre></div>
<pre><code>## [1] &quot;marginal likelihood = 0.0100&quot;</code></pre>
</div>
<div id="computing-the-posterior-1" class="section level3">
<h3><span class="header-section-number">11.4.5</span> Computing the posterior</h3>
<p>We now have all of the parts that we need to compute the posterior probability distribution across all possible values of <span class="math inline">\(p_{respond}\)</span>, as shown in Figure <a href="bayesian-statistics.html#fig:posteriorDist">11.2</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create data for use in figure</span>
bayesDf &lt;-
<span class="st">  </span><span class="kw">tibble</span>(
    <span class="dt">steps =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="fl">0.01</span>, <span class="dt">to =</span> <span class="fl">0.99</span>, <span class="dt">by =</span> <span class="fl">0.01</span>)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">likelihoods =</span> <span class="kw">dbinom</span>(
      <span class="dt">x =</span> nResponders, 
      <span class="dt">size =</span> <span class="dv">100</span>, 
      <span class="dt">prob =</span> steps
    ),
    <span class="dt">priors =</span> <span class="kw">dunif</span>(steps) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(steps),
    <span class="dt">posteriors =</span> (likelihoods <span class="op">*</span><span class="st"> </span>priors) <span class="op">/</span><span class="st"> </span>marginal_likelihood
  )</code></pre></div>
<div class="figure"><span id="fig:posteriorDist"></span>
<img src="StatsThinking21_files/figure-html/posteriorDist-1.png" alt="Posterior probability distribution plotted in blue against uniform prior distribution (dotted black line)." width="384" height="50%" />
<p class="caption">
Figure 11.2: Posterior probability distribution plotted in blue against uniform prior distribution (dotted black line).
</p>
</div>
</div>
<div id="maximum-a-posteriori-map-estimation" class="section level3">
<h3><span class="header-section-number">11.4.6</span> Maximum a posteriori (MAP) estimation</h3>
<p>Given our data we would like to obtain an estimate of <span class="math inline">\(p_{respond}\)</span> for our sample. One way to do this is to find the value of <span class="math inline">\(p_{respond}\)</span> for which the posterior probability is the highest, which we refer to as the <em>maximum a posteriori</em> (MAP) estimate. We can find this from the data in <a href="bayesian-statistics.html#fig:posteriorDist">11.2</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute MAP estimate</span>
MAP_estimate &lt;-<span class="st"> </span>
<span class="st">  </span>bayesDf <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(posteriors)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">pull</span>(steps)

<span class="kw">sprintf</span>(<span class="st">&quot;MAP estimate = %0.4f&quot;</span>, MAP_estimate)</code></pre></div>
<pre><code>## [1] &quot;MAP estimate = 0.6400&quot;</code></pre>
<p>Note that this is simply the proportion of responders from our sample – this occurs because the prior was uniform and thus didn’t bias our response.</p>
</div>
<div id="credible-intervals" class="section level3">
<h3><span class="header-section-number">11.4.7</span> Credible intervals</h3>
<p>Often we would like to know not just a single estimate for the posterior, but an interval in which we are confident that the posterior falls. We previously discussed the concept of confidence intervals in the context of frequentist inference, and you may remember that the interpretation of confidence intervals was particularly convoluted. What we really want is an interval in which we are confident that the true parameter falls, and Bayesian statistics can give us such an interval, which we call a <em>credible interval</em>.</p>
<p>In some cases the credible interval can be computed <em>numerically</em> based on a known distribution, but it’s more common to generate a credible interval by sampling from the posterior distribution and then to compute quantiles of the samples. This is particularly useful when we don’t have an easy way to express the posterior distribution numerically, which is often the case in real Bayesian data analysis.</p>
<p>We will generate samples from our posterior distribution using a simple algorithm known as <a href="https://am207.github.io/2017/wiki/rejectionsampling.html"><em>rejection sampling</em></a>. The idea is that we choose a random value of x (in this case <span class="math inline">\(p_{respond}\)</span>) and a random value of y (in this case, the posterior probability of <span class="math inline">\(p_{respond}\)</span>) each from a uniform distribution. We then only accept the sample if <span class="math inline">\(y &lt; f(x)\)</span> - in this case, if the randomly selected value of y is less than the actual posterior probability of y. Figure <a href="bayesian-statistics.html#fig:rejectionSampling">11.3</a> shows an example of a histogram of samples using rejection sampling, along with the 95% credible intervals obtained using this method.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compute credible intervals for example</span>

nsamples &lt;-<span class="st"> </span><span class="dv">100000</span>

<span class="co"># create random uniform variates for x and y</span>
x &lt;-<span class="st"> </span><span class="kw">runif</span>(nsamples)
y &lt;-<span class="st"> </span><span class="kw">runif</span>(nsamples)

<span class="co"># create f(x)</span>
fx &lt;-<span class="st"> </span><span class="kw">dbinom</span>(<span class="dt">x =</span> nResponders, <span class="dt">size =</span> <span class="dv">100</span>, <span class="dt">prob =</span> x)

<span class="co"># accept samples where y &lt; f(x)</span>
accept &lt;-<span class="st"> </span><span class="kw">which</span>(y <span class="op">&lt;</span><span class="st"> </span>fx)
accepted_samples &lt;-<span class="st"> </span>x[accept]

credible_interval &lt;-<span class="st"> </span><span class="kw">quantile</span>(<span class="dt">x =</span> accepted_samples, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))
<span class="kw">pander</span>(credible_interval)</code></pre></div>
<table style="width:19%;">
<colgroup>
<col width="9%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">2.5%</th>
<th align="center">98%</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.54</td>
<td align="center">0.72</td>
</tr>
</tbody>
</table>
<div class="figure"><span id="fig:rejectionSampling"></span>
<img src="StatsThinking21_files/figure-html/rejectionSampling-1.png" alt="Rejection sampling example.The black line shows the density of all possible values of p(respond); the blue lines show the 2.5th and 97.5th percentiles of the distribution, which represent the 95% credible interval for the estimate of p(respond)." width="384" height="50%" />
<p class="caption">
Figure 11.3: Rejection sampling example.The black line shows the density of all possible values of p(respond); the blue lines show the 2.5th and 97.5th percentiles of the distribution, which represent the 95% credible interval for the estimate of p(respond).
</p>
</div>
<p>The interpretation of this credible interval is much closer to what we had hoped we could get from a confidence interval (but could not): It tells us that there is a 95% probability that the value of <span class="math inline">\(p_{respond}\)</span> falls between these two values. Importantly, it shows that we have high confidence that <span class="math inline">\(p_{respond} &gt; 0.0\)</span>, meaning that the drug seems to have a positive effect.</p>
</div>
<div id="effects-of-different-priors" class="section level3">
<h3><span class="header-section-number">11.4.8</span> Effects of different priors</h3>
<p>In the previous example we used a <em>flat prior</em>, meaning that we didn’t have any reason to believe that any particular value of <span class="math inline">\(p_{respond}\)</span> was more or less likely. However, let’s say that we had instead started with some previous data: In a previous study, researchers had tested 20 people and found that 10 of them had responded positively. This would have lead us to start with a prior belief that the treatment has an effect in 50% of people. We can do the same computation as above, but using the information from our previous study to inform our prior (see Figure <a href="bayesian-statistics.html#fig:posteriorDistPrior">11.4</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute likelihoods for data under all values of p(heads) using a flat or empirical prior.  here we use the quantized values from .01 to .99 in steps of 0.01</span>

df &lt;-
<span class="st">  </span><span class="kw">tibble</span>(
    <span class="dt">steps =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="fl">0.01</span>, <span class="dt">to =</span> <span class="fl">0.99</span>, <span class="dt">by =</span> <span class="fl">0.01</span>)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">likelihoods =</span> <span class="kw">dbinom</span>(nResponders, <span class="dv">100</span>, steps),
    <span class="dt">priors_flat =</span> <span class="kw">dunif</span>(steps) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">dunif</span>(steps)),
    <span class="dt">priors_empirical =</span> <span class="kw">dbinom</span>(<span class="dv">10</span>, <span class="dv">20</span>, steps) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">dbinom</span>(<span class="dv">10</span>, <span class="dv">20</span>, steps))
  )

marginal_likelihood_flat &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">sum</span>(<span class="kw">dbinom</span>(nResponders, <span class="dv">100</span>, df<span class="op">$</span>steps) <span class="op">*</span><span class="st"> </span>df<span class="op">$</span>priors_flat)

marginal_likelihood_empirical &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">sum</span>(<span class="kw">dbinom</span>(nResponders, <span class="dv">100</span>, df<span class="op">$</span>steps) <span class="op">*</span><span class="st"> </span>df<span class="op">$</span>priors_empirical)

df &lt;-<span class="st"> </span>
<span class="st">  </span>df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">posteriors_flat =</span> 
      (likelihoods <span class="op">*</span><span class="st"> </span>priors_flat) <span class="op">/</span><span class="st"> </span>marginal_likelihood_flat,
    <span class="dt">posteriors_empirical =</span> 
      (likelihoods <span class="op">*</span><span class="st"> </span>priors_empirical) <span class="op">/</span><span class="st"> </span>marginal_likelihood_empirical
  )</code></pre></div>
<div class="figure"><span id="fig:posteriorDistPrior"></span>
<img src="StatsThinking21_files/figure-html/posteriorDistPrior-1.png" alt="Effects of priors on the posterior distribution.  The original posterior distribution based on a flat prior is plotted in blue. The prior based on the observation of 10 responders out of 20 people is plotted in the dotted black line, and the posterior using this prior is plotted in red." width="384" height="50%" />
<p class="caption">
Figure 11.4: Effects of priors on the posterior distribution. The original posterior distribution based on a flat prior is plotted in blue. The prior based on the observation of 10 responders out of 20 people is plotted in the dotted black line, and the posterior using this prior is plotted in red.
</p>
</div>
<p>Note that the likelihood and marginal likelihood did not change - only the prior changed. The effect of the change in prior to was to pull the posterior closer to the mass of the new prior, which is centered at 0.5.</p>
<p>Now let’s see what happens if we come to the analysis with an even stronger prior belief. Let’s say that instead of having previously observed 10 responders out of 20 people, the prior study had instead tested 500 people and found 250 responders. This should in principle give us a much stronger prior, and as we see in Figure <a href="bayesian-statistics.html#fig:strongPrior">11.5</a>, that’s what happens: The prior is much more concentrated around 0.5, and the posterior is also much closer to the prior. The general idea is that Bayesian inference combines the information from the prior and the likelihood, weighting the relative strength of each.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute likelihoods for data under all values of p(heads) using strong prior.</span>

df &lt;-
<span class="st">  </span>df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">priors_strong =</span> <span class="kw">dbinom</span>(<span class="dv">250</span>, <span class="dv">500</span>, steps) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">dbinom</span>(<span class="dv">250</span>, <span class="dv">500</span>, steps))
  )

marginal_likelihood_strong &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">sum</span>(<span class="kw">dbinom</span>(nResponders, <span class="dv">100</span>, df<span class="op">$</span>steps) <span class="op">*</span><span class="st"> </span>df<span class="op">$</span>priors_strong)

df &lt;-
<span class="st">  </span>df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">posteriors_strongprior =</span> (likelihoods <span class="op">*</span><span class="st"> </span>priors_strong) <span class="op">/</span><span class="st"> </span>marginal_likelihood_strong
  )</code></pre></div>
<div class="figure"><span id="fig:strongPrior"></span>
<img src="StatsThinking21_files/figure-html/strongPrior-1.png" alt="Effects of the strength of the prior on the posterior distribution. The blue line shows the posterior obtained using the prior based on 50 heads out of 100 people.  The dotted black line shows the prior based on 250 heads out of 500 flips, and the red line shows the posterior based on that prior." width="384" height="50%" />
<p class="caption">
Figure 11.5: Effects of the strength of the prior on the posterior distribution. The blue line shows the posterior obtained using the prior based on 50 heads out of 100 people. The dotted black line shows the prior based on 250 heads out of 500 flips, and the red line shows the posterior based on that prior.
</p>
</div>
<p>This example also highlights the sequential nature of Bayesian analysis – the posterior from one analysis can become the prior for the next analysis.</p>
<p>Finally, it is important to realize that if the priors are strong enough, they can completely overwhelm the data. Let’s say that you have an absolute prior that <span class="math inline">\(p_{respond}\)</span> is 0.8 or greater, such that you set the prior likelihood of all other values to zero. What happens if we then compute the posterior?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute likelihoods for data under all values of p(respond) using absolute prior. </span>
df &lt;-
<span class="st">  </span>df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">priors_absolute =</span> <span class="kw">array</span>(<span class="dt">data =</span> <span class="dv">0</span>, <span class="dt">dim =</span> <span class="kw">length</span>(steps)),
    <span class="dt">priors_absolute =</span> <span class="kw">if_else</span>(
      steps <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.8</span>,
      <span class="dv">1</span>, priors_absolute
    ),
    <span class="dt">priors_absolute =</span> priors_absolute <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(priors_absolute)
  )

marginal_likelihood_absolute &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">sum</span>(<span class="kw">dbinom</span>(nResponders, <span class="dv">100</span>, df<span class="op">$</span>steps) <span class="op">*</span><span class="st"> </span>df<span class="op">$</span>priors_absolute)

df &lt;-
<span class="st">  </span>df <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">posteriors_absolute =</span> 
      (likelihoods <span class="op">*</span><span class="st"> </span>priors_absolute) <span class="op">/</span><span class="st"> </span>marginal_likelihood_absolute
  )</code></pre></div>
<div class="figure"><span id="fig:absolutePrior"></span>
<img src="StatsThinking21_files/figure-html/absolutePrior-1.png" alt="Effects of the strength of the prior on the posterior distribution. The blue line shows the posterior obtained using an absolute prior which states that p(respond) is 0.8 or greater.  The prior is shown in the dotted red line." width="384" height="50%" />
<p class="caption">
Figure 11.6: Effects of the strength of the prior on the posterior distribution. The blue line shows the posterior obtained using an absolute prior which states that p(respond) is 0.8 or greater. The prior is shown in the dotted red line.
</p>
</div>
<p>In Figure <a href="bayesian-statistics.html#fig:absolutePrior">11.6</a> we see that there is zero density in the posterior for any of the values where the prior was set to zero - the data are overwhelmed by the absolute prior.</p>
</div>
</div>
<div id="choosing-a-prior" class="section level2">
<h2><span class="header-section-number">11.5</span> Choosing a prior</h2>
<p>The impact of priors on the resulting inferences are the most controversial aspect of Bayesian statistics. There are various ways to choose one’s priors, which (as we saw above) can impact the resulting inferences. <em>Uninformative priors</em> attempt to bias the resulting posterior as little as possible, as we saw in the example of the uniform prior above. It’s also common to use <em>weakly informative priors</em> (or <em>default priors</em>), which bias the result only very slightly. For example, if we had used a binomial distribution based on one heads out of two coin flips, the prior would have been centered around 0.5 but fairly flat, biasing the posterior only slightly.</p>
<p>It is also possible to use priors based on the scientific literature or pre-existing data, which we would call <em>empirical priors</em>. In general, however, we will stick to the use of uninformative/weakly informative priors, since they raise the least concern about biasing our results. In general it is a good idea to try any Bayesian analysis using multiple reasonable priors, and make sure that the results don’t change in an important way based on the prior.</p>
</div>
<div id="bayesian-hypothesis-testing" class="section level2">
<h2><span class="header-section-number">11.6</span> Bayesian hypothesis testing</h2>
<p>Having learned how to perform Bayesian estimation, we now turn to the use of Bayesian methods for hypothesis testing. Let’s say that there are two politicians who differ in their beliefs about whether the public supports the death penalty. Senator Smith thinks that only 40% of people support the death penalty, whereas Senator Jones thinks that 60% of people support the death penalty. They arrange to have a poll done to test this, which asks 1000 randomly selected people whether they support the death penalty. The results are that 490 of the people in the polled sample support the death penalty. Based on these data, we would like to know: Do the data support the claims of one senator over the other? We can test this using a concept known as the <a href="https://bayesfactor.blogspot.com/2014/02/the-bayesfactor-package-this-blog-is.html">Bayes factor</a>.</p>
<div id="bayes-factors" class="section level3">
<h3><span class="header-section-number">11.6.1</span> Bayes factors</h3>
<p>The Bayes factor characterizes the relative likelihood of the data under two different hypotheses. It is defined as:</p>
<p><span class="math display">\[
BF = \frac{p(data|H_1)}{p(data|H_2)}
\]</span> for two hypotheses <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span>. In the case of our two senators, we know how to compute the likelihood of the data under each hypothesis using the binomial distribution. We will put Senator Smith in the numerator and Senator Jones in the denominator, so that a value greater than one will reflect greater evidence for Senator Smith, and a value less than one will reflect greater evidence for Senator Jones.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute Bayes factor for Smith vs. Jones</span>

bf &lt;-
<span class="st">  </span><span class="kw">dbinom</span>(
    <span class="dt">x =</span> <span class="dv">490</span>,
    <span class="dt">size =</span> <span class="dv">1000</span>,
    <span class="dt">prob =</span> <span class="fl">0.4</span> <span class="co">#Smith&#39;s hypothesis</span>
  ) <span class="op">/</span><span class="st"> </span><span class="kw">dbinom</span>(
    <span class="dt">x =</span> <span class="dv">490</span>, 
    <span class="dt">size =</span> <span class="dv">1000</span>, 
    <span class="dt">prob =</span> <span class="fl">0.6</span> <span class="co">#Jones&#39; hypothesis</span>
  )

<span class="kw">sprintf</span>(<span class="st">&quot;Bayes factor = %0.2f&quot;</span>, bf)</code></pre></div>
<pre><code>## [1] &quot;Bayes factor = 3325.26&quot;</code></pre>
<p>This number provides a measure of the evidence that the data provides regarding the two hypotheses - in this case, it tells us the data support Senator Smith more than 3000 times more strongly than they support Senator Jones.</p>
</div>
<div id="bayes-factors-for-statistical-hypotheses" class="section level3">
<h3><span class="header-section-number">11.6.2</span> Bayes factors for statistical hypotheses</h3>
<p>In the previous example we had specific predictions from each senator, whose likelihood we could quantify using the binomial distribution. However, in real data analysis we generally must deal with uncertainty about our parameters, which complicates the Bayes factor. However, in exchange we gain the ability to quantify the relative amount of evidence in favor of the null versus alternative hypotheses.</p>
<p>Let’s say that we are a medical researcher performing a clinical trial for the treatment of diabetes, and we wish to know whether a particular drug reduces blood glucose compared to placebo. We recruit a set of volunteers and randomly assign them to either drug or placebo group, and we measure the change in hemoglobin A1C (a marker for blood glucose levels) in each group over the period in which the drug or placebo was administered. What we want to know is: Is there a difference between the drug and placebo?</p>
<p>First, let’s generate some data and analyze them using null hypothesis testing (see Figure <a href="bayesian-statistics.html#fig:bayesTesting">11.7</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create simulated data for drug trial example</span>

<span class="kw">set.seed</span>(<span class="dv">123456</span>)
nsubs &lt;-<span class="st"> </span><span class="dv">40</span>
effect_size &lt;-<span class="st"> </span><span class="fl">0.1</span>

<span class="co"># randomize indiviuals to drug (1) or placebo (0)</span>
drugDf &lt;-
<span class="st">  </span><span class="kw">tibble</span>(
    <span class="dt">group =</span> <span class="kw">as.integer</span>(<span class="kw">runif</span>(nsubs) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>)
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">hbchange =</span> <span class="kw">rnorm</span>(nsubs) <span class="op">-</span><span class="st"> </span>group <span class="op">*</span><span class="st"> </span>effect_size
  )</code></pre></div>
<div class="figure"><span id="fig:bayesTesting"></span>
<img src="StatsThinking21_files/figure-html/bayesTesting-1.png" alt="Box plots showing data for drug and placebo groups." width="384" height="50%" />
<p class="caption">
Figure 11.7: Box plots showing data for drug and placebo groups.
</p>
</div>
<p>Let’s perform an independent-samples t-test, which shows that there is a significant difference between the groups:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute t-test for drug example</span>
drugTT &lt;-<span class="st"> </span><span class="kw">t.test</span>(hbchange <span class="op">~</span><span class="st"> </span>group, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>, <span class="dt">data =</span> drugDf)
<span class="kw">print</span>(drugTT)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  hbchange by group
## t = 2, df = 40, p-value = 0.03
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  0.096   Inf
## sample estimates:
## mean in group 0 mean in group 1 
##            0.12           -0.48</code></pre>
<p>This test tells us that there is a significant difference between the groups, but it doesn’t quantify how strongly the evidence supports the null versus alternative hypotheses. To measure that, we can compute a Bayes factor using <code>ttestBF</code> function from the BayesFactor package in R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute Bayes factor for drug data</span>
bf_drug &lt;-<span class="st"> </span><span class="kw">ttestBF</span>(
  <span class="dt">formula =</span> hbchange <span class="op">~</span><span class="st"> </span>group, <span class="dt">data =</span> drugDf,
  <span class="dt">nullInterval =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="ot">Inf</span>)
)

bf_drug</code></pre></div>
<pre><code>## Bayes factor analysis
## --------------
## [1] Alt., r=0.707 0&lt;d&lt;Inf    : 2.4  ±0%
## [2] Alt., r=0.707 !(0&lt;d&lt;Inf) : 0.12 ±0%
## 
## Against denominator:
##   Null, mu1-mu2 = 0 
## ---
## Bayes factor type: BFindepSample, JZS</code></pre>
<p>The Bayes factor here tells us that the alternative hypothesis (i.e. that the difference is greater than zero) is about 2.4 times more likely than the null hypothesis given the data. This doesn’t seem that strong, but how can we determine that? There is a general guideline for interpretation of Bayes factors suggested by <a href="https://www.andrew.cmu.edu/user/kk3n/simplicity/KassRaftery1995.pdf">Kass &amp; Rafferty (1995)</a>:</p>
<table>
<thead>
<tr class="header">
<th>BF</th>
<th>Strength of evidence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1 to 3</td>
<td>not worth more than a bare mention</td>
</tr>
<tr class="even">
<td>3 to 20</td>
<td>positive</td>
</tr>
<tr class="odd">
<td>20 to 150</td>
<td>strong</td>
</tr>
<tr class="even">
<td>&gt;150</td>
<td>very strong</td>
</tr>
</tbody>
</table>
<p>Based on this, even though the statisical result is significant, the amount of evidence in favor of the alternative vs. the null hypothesis is weak enough that it’s not worth even mentioning.</p>
<p>Because the Bayes factor is comparing evidence for two hypotheses, it also allows us to assess whether there is evidence in favor of the null hypothesis, which we couldn’t do with standard null hypothesis testing (because it starts with the assumption that the null is true). This can be very useful for determining whether a non-significant result really provides strong evidence that there is no effect, or instead just reflects weak evidence overall.</p>
</div>
</div>
<div id="suggested-readings-8" class="section level2">
<h2><span class="header-section-number">11.7</span> Suggested readings</h2>
<ul>
<li><em>The Theory That Would Not Die: How Bayes’ Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy</em>, by Sharon Bertsch McGrayne</li>
<li><em>Doing Bayesian Data Analysis: A Tutorial Introduction with R</em>, by John K. Kruschke</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ci-effect-size-power.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modeling-categorical-relationships.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/poldrack/psych10-book/edit/master/11-BayesianStatistics.Rmd",
"text": "Edit"
},
"download": ["StatsThinking21.pdf", "StatsThinking21.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
